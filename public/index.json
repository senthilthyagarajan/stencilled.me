[{"content":"QuickMMM: Making Media Mix Modeling Simple for Marketers In today\u0026rsquo;s fast-moving marketing world, waiting 5 months and spending $200K+ on traditional Media Mix Modeling (MMM) just doesn\u0026rsquo;t work.\nEnter QuickMMM — a fast, approachable tool that gives marketers media insights in hours, not months.\nTry it now 👉 mmm-turbo-insights.lovable.app\n💡 The Problem: Traditional MMM Is Too Slow, Too Expensive Takes too long Costs too much Delivers insights when it\u0026rsquo;s too late to act 🧠 What is QuickMMM? QuickMMM is a lightweight, web-based Media Mix Modeling tool built to help marketers understand which channels are really driving results.\nNo advanced modeling. No waiting months. Just your weekly spend and conversions — turned into clear, visual insights.\n🔧 Core Features 📊 Channel contribution analysis 📉 Diminishing returns visualization 🔁 Budget reallocation suggestions 💰 ROI optimization insights ⚙️ Powered By Trusted Methods Meta\u0026rsquo;s Robyn framework Google\u0026rsquo;s Lightweight MMM Ridge Regression + Prophet Bayesian Hierarchical Modeling 🖼️ QuickMMM in Action Main Dashboard Upload your CSV. Choose your model. View insights instantly.\nChannel Analysis Break down conversion impact by channel.\nROI Insights Spot when you\u0026rsquo;re overspending with clear saturation curves.\nDesigned for marketers — not data scientists.\n🎯 Why QuickMMM? Traditional MMM QuickMMM 🕰️ 5+ Month Timeline ⚡ 1-Hour Setup 💸 $200K+ Investment 💼 Accessible for All Teams 🧠 Complex Outputs 🔍 Marketer-Friendly Visuals ⚡ Key Advantages Speed \u0026amp; Agility\nMake smarter decisions while campaigns are still live.\nCost-Effective\nGet ~80% of the value at 5% of the cost.\nExplainable Models\nUnderstand how and why the results were calculated.\nMultiple Modeling Options\nChoose from Robyn (fast) or LMMM (deep + Bayesian).\nSelf-Serve Interface\nNo consultants. No setup calls. Just insights.\nVisual by Design\nInteractive charts make your media story easy to understand.\n⚠️ Important Caveats \u0026amp; Limitations 📊 Data Quality Requirements Minimum 12–20 weeks of clean data Weekly or monthly granularity Channel-level spend and conversion data required 🧪 Statistical Notes Shows correlation-based insights, not strict causation Works best for directional guidance, not precise lift measurement Doesn\u0026rsquo;t replace attribution or holdout testing Use QuickMMM for rapid optimization — not for legal or finance-grade forecasting.\n🎨 Best Use Cases Mid-Campaign Optimization\nQuickly reallocate underperforming spend.\nQuarterly Budget Planning\nUse historical patterns to inform next quarter\u0026rsquo;s mix.\nLeadership Reporting\nTurn your numbers into clear, story-driven visual summaries.\n🔍 Under the Hood 🔵 Meta\u0026rsquo;s Robyn Ridge/Lasso Regression Prophet seasonality Geometric Adstock Hill saturation curves 🟢 Google\u0026rsquo;s Lightweight MMM Bayesian inference (Stan or PyMC) Hierarchical modeling Flexible priors Full posterior distributions 🚀 Ready to Try It? Try the live app now 👉 https://mmm-turbo-insights.lovable.app\nHow It Works: Upload Your Data\nSimple CSV with dates, spend, and conversions\nChoose a Model\nRobyn for fast results, LMMM for detailed modeling\nGet Instant Insights\nVisuals and recommendations in minutes\n💬 Have feedback or want help getting started?\n👉 Reach out via LinkedIn: www.linkedin.com/in/senthilthyagarajan\n","permalink":"http://www.stencilled.me/posts/2025-06-10-quickmmm/","summary":"QuickMMM: Making Media Mix Modeling Simple for Marketers In today\u0026rsquo;s fast-moving marketing world, waiting 5 months and spending $200K+ on traditional Media Mix Modeling (MMM) just doesn\u0026rsquo;t work.\nEnter QuickMMM — a fast, approachable tool that gives marketers media insights in hours, not months.\nTry it now 👉 mmm-turbo-insights.lovable.app\n💡 The Problem: Traditional MMM Is Too Slow, Too Expensive Takes too long Costs too much Delivers insights when it\u0026rsquo;s too late to act 🧠 What is QuickMMM?","title":"QuickMMM: Making Media Mix Modeling Simple for Marketers"},{"content":"It always starts the same way.\nA user types:\n\u0026ldquo;What\u0026rsquo;s the best multivitamin for women in their 30s?\u0026rdquo;\n\u0026ldquo;Healthiest soda alternative?\u0026rdquo;\n\u0026ldquo;Best travel insurance for a family trip to Italy?\u0026rdquo;\nBut they don\u0026rsquo;t click a search result. They don\u0026rsquo;t scroll through 10 blue links. They ask ChatGPT, Perplexity, or Gemini, and they get a confident, summarized answer.\nIf your brand isn\u0026rsquo;t mentioned in that response, you don\u0026rsquo;t exist in that moment.\nAnd here\u0026rsquo;s the kicker: if your website isn\u0026rsquo;t built for LLMs, your analytics team is tracking the wrong funnel.\nThe Funnel Is Changing — But Our Metrics Aren\u0026rsquo;t Most websites—and their analytics frameworks—are still stuck in an old paradigm:\nShow an ad Drive the click Land the user on a homepage Navigate them through content Convert This model assumes a human is your first touchpoint. But that\u0026rsquo;s no longer true.\nLLMs don\u0026rsquo;t click. They read. They crawl. They extract.\nThey summarize your content—sometimes better than your own site explains it.\nAnd if your content isn\u0026rsquo;t structured clearly and contextually, it won\u0026rsquo;t make the cut in those machine-generated answers.\nA Real-World Pattern: Looks Great, Says Nothing Take, for example, a health and wellness brand that recently launched a visually impressive website. It had everything — animations, bold taglines, lifestyle imagery.\nBut when someone asked ChatGPT,\n\u0026ldquo;What\u0026rsquo;s the best probiotic for women over 40?\u0026rdquo;\nThe brand didn\u0026rsquo;t appear in the response.\nDigging deeper, the product information was embedded in images, key benefits were vague, and there were no structured FAQs or direct answers. The homepage looked great but didn\u0026rsquo;t clearly state what the product did, who it was for, or how it stood out.\nFrom a traditional analytics standpoint, traffic looked healthy, but bounce rates were high and conversion rates underwhelming.\nThe problem wasn\u0026rsquo;t just UX — it was machine unreadability. The site wasn\u0026rsquo;t designed to be understood and cited by an LLM. So it got skipped entirely.\nYour Funnel Looks Great—But Not to an LLM 🧱 1. Structure Matters More Than Ever Use clear headers (\u0026lt;h1\u0026gt;, \u0026lt;h2\u0026gt;), clean HTML, and schema markup.\nLLMs rely on structure to understand your content—if your product info is buried in sliders, hidden in images, or scattered across tabs, it might not get picked up at all.\nStructure isn\u0026rsquo;t just for SEO anymore.\nIt\u0026rsquo;s the foundation that helps machines interpret your site and confidently include it in answers.\nThink of it as making your site easier to read, not just to look at.\n✍️ 2. Make Metadata Matter Again Your meta descriptions, alt text, and OpenGraph tags might seem minor. But they\u0026rsquo;re often the first text LLMs grab.\nTreat these like your site\u0026rsquo;s elevator pitch to an algorithm.\n❓ 3. Build for Questions, Not Just Pages Rather than generic pages like \u0026ldquo;About Us\u0026rdquo; or \u0026ldquo;Our Story,\u0026rdquo; consider content that answers:\n\u0026ldquo;Is this product vegan?\u0026rdquo; \u0026ldquo;Can I use this with a gluten allergy?\u0026rdquo; \u0026ldquo;Does it work for postpartum women?\u0026rdquo; If people are asking, LLMs are too.\nFrom Click Paths to Machine Confidence Traditionally, your analytics dashboard tells you:\nPageviews Bounce rate Time on site But in an LLM world, that\u0026rsquo;s only half the picture.\nNow you should be asking:\nDid this page answer a real question? Could a language model confidently summarize our offering? Are we showing up in responses from ChatGPT, Perplexity, or Gemini? Some brands are already starting to track these LLM citations. And soon, \u0026ldquo;Are we quoted?\u0026rdquo; will matter just as much as \u0026ldquo;Are we ranked?\u0026rdquo;\nRebuilding the Funnel Around LLMs LLMs are reshaping the entire journey:\n🎯 Upper Funnel (Awareness) LLMs summarize and recommend. If your content isn\u0026rsquo;t clearly written and structured, it won\u0026rsquo;t get surfaced.\n🔍 Mid-Funnel (Consideration) Users ask for comparisons, side effects, ingredients, or reviews. LLMs want to cite trustworthy, well-laid-out answers.\n💳 Lower Funnel (Conversion) Some LLMs may eventually link directly to products. If your landing page isn\u0026rsquo;t clear, fast, and complete—it won\u0026rsquo;t make the cut.\nWhat Should Analytics Teams Do Now? If you\u0026rsquo;re leading analytics or measurement at a brand, here\u0026rsquo;s your new checklist:\n✅ Review how your product and landing pages are structured ✅ Audit for schema, metadata, and semantic clarity ✅ Track whether your brand appears in LLM answers ✅ Collaborate with SEO and content teams for machine readability ✅ Update reporting to include LLM visibility and citation metrics Final Thought: Build for Clicks. Build for Quotes. In 2025, your website has two front doors:\nThe human visitor The LLM query If your site only welcomes one, you\u0026rsquo;re leaving opportunity—and revenue—on the table.\nThe brands that thrive will be those that educate both people and machines, and measure not just who clicks, but who quotes.\n👀 Try This Ask ChatGPT or Perplexity:\n\u0026ldquo;What\u0026rsquo;s the best [category] for [audience]?\u0026rdquo;\nIf your brand doesn\u0026rsquo;t make the list, it might be time for a site refresh—not just another campaign.\n","permalink":"http://www.stencilled.me/posts/2025-06-09-llm-funnel/","summary":"It always starts the same way.\nA user types:\n\u0026ldquo;What\u0026rsquo;s the best multivitamin for women in their 30s?\u0026rdquo;\n\u0026ldquo;Healthiest soda alternative?\u0026rdquo;\n\u0026ldquo;Best travel insurance for a family trip to Italy?\u0026rdquo;\nBut they don\u0026rsquo;t click a search result. They don\u0026rsquo;t scroll through 10 blue links. They ask ChatGPT, Perplexity, or Gemini, and they get a confident, summarized answer.\nIf your brand isn\u0026rsquo;t mentioned in that response, you don\u0026rsquo;t exist in that moment.","title":"Is Your Website Built for LLMs? Why Analytics Needs to Rethink the Funnel"},{"content":"It always starts the same way. A campaign wraps, and the team pulls up the performance report. The click-through rate? Above benchmark. CPM? 30% lower than the platform average. There\u0026rsquo;s a general sense of relief, maybe even celebration. Everyone\u0026rsquo;s nodding. Job well done. But then someone asks the one question that doesn\u0026rsquo;t show up on the dashboard: \u0026ldquo;Did it actually drive anything?\u0026rdquo;\nThat\u0026rsquo;s when things get murky.\nBenchmarks: The Rental Tux That Sort of Fits Benchmarks have become a go-to comfort metric in media analytics. They\u0026rsquo;re quick to reference, look clean in a deck, and offer a sense of relative performance. But relying on them too heavily is like judging how well a tuxedo fits based on how it looks on a mannequin. Sure, it might look good at first glance, but it wasn\u0026rsquo;t made for you. It wasn\u0026rsquo;t designed for your body, your movement, or your context.\nPlatform benchmarks from Meta, TikTok, YouTube, and others are built on data from thousands of campaigns—across every imaginable industry, audience, budget, and creative format. They\u0026rsquo;re directionally helpful, but ultimately impersonal. A benchmark might tell you your CPM is in the \u0026ldquo;top quartile,\u0026rdquo; but it won\u0026rsquo;t tell you if the impressions actually made a difference. It won\u0026rsquo;t tell you if your creative landed, or if you moved anyone meaningfully through the funnel. That\u0026rsquo;s why they\u0026rsquo;re often misleading when used in isolation.\nBaselines: The Hoodie That Learns You Now compare that to baselines. A baseline is your favorite hoodie. You\u0026rsquo;ve worn it a hundred times. It fits better every time you put it on—not because it was expensive, but because it knows you. In the same way, a baseline is built from your past performance data. It reflects how your campaigns have actually performed in the real world—with your brand, your audiences, your creative, and your spend levels.\nWhere benchmarks are generic and one-size-fits-most, baselines are personal. They allow you to say, \u0026ldquo;When we run $75K on Meta using UGC content in Q4, we typically see a 1.2% CTR and a $7.50 CPL.\u0026rdquo; That\u0026rsquo;s not just a reference point—it\u0026rsquo;s a performance memory that helps you plan more effectively and realistically. It turns your historical data into a predictive tool. And when used consistently, it becomes a much more powerful decision-making engine than any external benchmark.\nThe Deceptive CPM Here\u0026rsquo;s a real-world example of where benchmarks can lead you astray. A consumer brand recently ran a Connected TV campaign. On paper, it looked like a win—the CPM came in 30% below the industry benchmark. The team called it a success and moved on. But when the placements were reviewed more closely, the truth came out: most of the impressions ran in overnight slots. Low viewability, no engagement, and no brand recall.\nYes, it was \u0026ldquo;efficient.\u0026rdquo; But it was also forgettable. The campaign delivered impressions, not impact. This is the danger of evaluating success based purely on a cost metric. When you use benchmarks as a substitute for real effectiveness, you can easily mistake cheap delivery for meaningful performance.\nSo How Should You Use Benchmarks? Benchmarks aren\u0026rsquo;t inherently bad. They\u0026rsquo;re useful for orientation—especially when you\u0026rsquo;re entering a new platform, testing a new format, or setting initial expectations. The problem is when they become the only frame of reference. The best use of benchmarks is as a complement to your own data, not a replacement for it.\nIf you\u0026rsquo;re comparing campaign performance, make sure the benchmark is even relevant. Was it measured on the same objective? Was the creative type similar? Was the spend in the same ballpark? If not, you\u0026rsquo;re comparing apples to something else entirely. Always layer your benchmarks with baselines. Let your internal data guide the narrative. If your own performance was down compared to last quarter—even if you beat the platform average—that\u0026rsquo;s a red flag, not a reason to celebrate.\nAnd above all, don\u0026rsquo;t confuse \u0026ldquo;above average\u0026rdquo; with \u0026ldquo;successful.\u0026rdquo; A high CTR that leads to no action, or a low CPM that reaches the wrong audience, doesn\u0026rsquo;t help your brand grow.\nBenchmarks Are One-Dimensional. Baselines Are Multi-Layered. Benchmarks give you a snapshot. Baselines give you a timeline. Benchmarks tell you how you stack up against others. Baselines tell you how you\u0026rsquo;re progressing over time. One is outward-facing; the other is rooted in your own evolution. Both have their place, but only one helps you get better in a way that matters.\nWant to Go Deeper? If this resonates, check out this excellent episode from The Analytics Power Hour:\n🎧 #254: Is Your Use of Benchmarks Above Average? featuring Eric Sandosham.\nIt\u0026rsquo;s a thoughtful, clear-headed take on how analytics teams can stop chasing \u0026ldquo;average\u0026rdquo; and start building measurement frameworks that actually reflect reality.\nTakeaway? Benchmarks can help you start the conversation. But baselines will help you finish it. A rental tux might look good in a photo, but a hoodie that fits is what you\u0026rsquo;ll reach for when things get real.\nNext time someone says, \u0026ldquo;We beat the benchmark,\u0026rdquo; ask a better question:\nDid we beat ourselves at our best?\n","permalink":"http://www.stencilled.me/posts/2025-06-02-benchmarks-baselines/","summary":"It always starts the same way. A campaign wraps, and the team pulls up the performance report. The click-through rate? Above benchmark. CPM? 30% lower than the platform average. There\u0026rsquo;s a general sense of relief, maybe even celebration. Everyone\u0026rsquo;s nodding. Job well done. But then someone asks the one question that doesn\u0026rsquo;t show up on the dashboard: \u0026ldquo;Did it actually drive anything?\u0026rdquo;\nThat\u0026rsquo;s when things get murky.\nBenchmarks: The Rental Tux That Sort of Fits Benchmarks have become a go-to comfort metric in media analytics.","title":"Benchmarks vs. Baselines: When 'Above Average' Isn't Good Enough"},{"content":"Stop Asking for Data You\u0026rsquo;ll Never Use The Real Problem Isn\u0026rsquo;t the Dashboard. It\u0026rsquo;s How We Treat Data as a Checklist, Not a Conversation.\nAnalytics teams are constantly asked for everything: \u0026ldquo;Can you pull media spend, website traffic, sales lift, creative performance, even click text and session duration?\u0026rdquo;\nAnd they deliver. Hours go into cleaning data, joining tables, building charts, uploading decks, and updating dashboards — most of which barely get used.\nSound familiar?\nThe truth is: data is abundant. Insights are not. And often, the data ends up as window dressing — skimmed by someone with partial context, leading to surface-level takeaways like \u0026ldquo;CTR dropped, let\u0026rsquo;s pull back spend,\u0026rdquo; instead of a deeper understanding of why performance changed.\nWhere Things Go Wrong 1. Data Is Treated Like a Deliverable, Not a Dialogue Data requests are treated as transactions — \u0026ldquo;Send me the dashboard,\u0026rdquo; \u0026ldquo;Pull this report.\u0026rdquo; But without shared context around campaign goals or audience behavior, the numbers mean little — or worse, they\u0026rsquo;re misread.\n2. Dashboards Exist, But Rarely Get Used There are dashboards in Looker, Tableau, GA4, Google Sheets. Yet logins stay flat. Why? Because static dashboards rarely answer real questions like:\n\u0026ldquo;What message drove qualified traffic?\u0026rdquo;\n\u0026ldquo;Did spend shifts actually influence downstream results?\u0026rdquo;\n3. Platform Fluency Without Data Fluency Leads to Misreads Sometimes, data ends up being interpreted by those fluent in channels but not analytics. A drop in engagement doesn\u0026rsquo;t always mean the creative failed. Maybe frequency was too high. Maybe the CTA didn\u0026rsquo;t match landing page intent. That nuance gets lost.\nSo How Do We Fix This? We don\u0026rsquo;t need more dashboards. We need a stronger feedback loop between analytics and decision-making.\n✅ 1. Ask Better Questions Before the Data Pull Instead of \u0026ldquo;Can I get traffic by campaign?\u0026rdquo; ask:\n\u0026ldquo;Did people who clicked this creative take any meaningful action onsite?\u0026rdquo;\nThat one shift turns a report into an insight.\n✅ 2. Align on What \u0026lsquo;Good\u0026rsquo; Looks Like If the ultimate goal is sales lift, don\u0026rsquo;t optimize for CTR mid-flight. Align on KPIs and ladder them clearly: from impressions to engagement to qualified sessions to sales.\n✅ 3. Reserve Analytics Time for Exploration, Not Repetition Automate repetitive pulls using Supermetrics, BigQuery, or Apps Script. Use that time to dig deeper — into why something happened, not just what.\n✅ 4. Make Data a Conversation, Not a Chart Add narration. Start insight threads in Slack. Hold monthly \u0026ldquo;insight sprints.\u0026rdquo; Treat analytics like a product: interactive, iterative, and owned by the whole team — not just the analysts.\nClosing Thought Data abundance doesn\u0026rsquo;t mean insight maturity. If analysts are spending hours on dashboards no one logs into, and the same questions keep getting asked — something\u0026rsquo;s broken.\nLet\u0026rsquo;s fix it. Not by doing less, but by making analytics work harder — for the right questions.\n","permalink":"http://www.stencilled.me/posts/2025-05-06-stop-asking-data/","summary":"Stop Asking for Data You\u0026rsquo;ll Never Use The Real Problem Isn\u0026rsquo;t the Dashboard. It\u0026rsquo;s How We Treat Data as a Checklist, Not a Conversation.\nAnalytics teams are constantly asked for everything: \u0026ldquo;Can you pull media spend, website traffic, sales lift, creative performance, even click text and session duration?\u0026rdquo;\nAnd they deliver. Hours go into cleaning data, joining tables, building charts, uploading decks, and updating dashboards — most of which barely get used.","title":"Stop Asking for Data You'll Never Use"},{"content":"Why Every Media Team Needs a Measurement Playbook (Not Just a Dashboard) If you\u0026rsquo;ve ever felt lost in the maze of media analytics jargon—MMM, incrementality, geo-testing—you\u0026rsquo;re not alone. But what if I told you these aren\u0026rsquo;t just buzzwords; they\u0026rsquo;re your roadmap to mastering media performance? Let\u0026rsquo;s dive in and demystify how these analytics tools actually work—and how you can leverage them to move from tactical execution to strategic mastery.\nMarketing Mix Modeling (MMM): Beyond the Rear-view Mirror MMM might sound like staring at your past data through a foggy window. But today\u0026rsquo;s MMM is more like having a GPS guiding your marketing spend in real-time. It shows you how effectively each dollar spent contributes to your overall sales, not just historically but dynamically.\nHow MMM Really Works (Simplified):\nGetting Started (Excel Method):\nGather your historical sales and spend data. Use Excel\u0026rsquo;s regression tools to understand basic channel ROI and effectiveness. Taking it Up a Notch:\nAd-stock Modeling: Think of it as your ads having memory—ad-stock modeling tracks how your ads continue impacting audiences long after airing. Incrementality Testing: Your Media Detective Incrementality testing isn\u0026rsquo;t just another metric—it\u0026rsquo;s your secret investigative tool to reveal the true impact of your marketing efforts. It directly measures the incremental effect your campaigns have by comparing real-world outcomes.\nEasy Incrementality Testing (Excel/Google Sheets):\nSelect two groups—one sees your ads (test), one doesn\u0026rsquo;t (control).\nTrack conversions separately for straightforward, actionable insights.\nExciting Types of Incrementality Tests:\nGeo Testing: Find out why your campaign crushes it in California but stumbles in Florida. Matched Market Testing: Pinpoint insights by comparing similar markets, like detective work for consumer behaviors. Conversion Lift Studies: Instantly check if your digital ads actually push consumers closer to buying. Scaling Up Incrementality (With Some Analytical Magic) Think of scaling up incrementality tests as upgrading your analytical team with smart assistants:\nAdvanced Platforms: Like reliable and precise assistants—tools like Google\u0026rsquo;s CausalImpact or Facebook\u0026rsquo;s GeoLift automate and scale your experiments effortlessly. Machine Learning-Enhanced Testing: Imagine having interns that continually learn and adjust your testing strategies dynamically, pinpointing exactly what\u0026rsquo;s most effective. Predictive Insights: Combine test results with predictive modeling, helping you confidently forecast future performance and spending. MMM vs. Incrementality: Choosing Your Analytical Ally MMM is perfect when:\nYou\u0026rsquo;re aiming for long-term strategic insights. You want to understand the broader dynamics and relationships across channels. Incrementality is your go-to when:\nQuick, actionable insights are crucial. You need undeniable proof of a campaign\u0026rsquo;s direct impact. Bringing It All Together When you blend MMM\u0026rsquo;s strategic insights with incrementality\u0026rsquo;s real-world detective work, you\u0026rsquo;re not just crunching numbers—you\u0026rsquo;re transforming analytics into actionable wisdom. It\u0026rsquo;s like having a crystal ball backed by hard evidence, empowering you to predict trends, optimize budgets, and confidently guide your brand through the ever-changing landscape of media performance.\n","permalink":"http://www.stencilled.me/posts/2024-05-08-measurement-playbook/","summary":"Why Every Media Team Needs a Measurement Playbook (Not Just a Dashboard) If you\u0026rsquo;ve ever felt lost in the maze of media analytics jargon—MMM, incrementality, geo-testing—you\u0026rsquo;re not alone. But what if I told you these aren\u0026rsquo;t just buzzwords; they\u0026rsquo;re your roadmap to mastering media performance? Let\u0026rsquo;s dive in and demystify how these analytics tools actually work—and how you can leverage them to move from tactical execution to strategic mastery.\nMarketing Mix Modeling (MMM): Beyond the Rear-view Mirror MMM might sound like staring at your past data through a foggy window.","title":"Why Every Media Team Needs a Measurement Playbook (Not Just a Dashboard)"},{"content":"Why Every Media Team Needs an Analytics Agent From dashboards to decisions — AI is now the analyst.\n🧠 The \u0026ldquo;What\u0026rsquo;s Our Best Channel?\u0026rdquo; Moment Imagine this:\nA strategist opens Slack on Monday morning and types:\n\u0026ldquo;What\u0026rsquo;s our best-performing channel in Q2 for CPL under $20?\u0026rdquo;\nSeconds later, the reply pops up:\n🏆 Top channel: Meta (CPL: $18.67, CTR: 1.3%) ⚠️ TikTok pacing 22% behind plan ✅ Google leads in conversion rate (6.1%) No dashboard. No pivot table. No email thread.\nJust… answers.\nThis isn\u0026rsquo;t sci-fi or next year\u0026rsquo;s roadmap.\nIt\u0026rsquo;s already happening — and it\u0026rsquo;s called an analytics agent.\n🤖 What Is an Analytics Agent? An analytics agent is an AI-powered teammate that can:\nUnderstand performance questions Pull metrics from your media stack (BigQuery, Google Sheets, Looker) Interpret KPIs and context (benchmarks, funnel stages, pacing) Communicate insights in human language — on Slack, email, or slides It\u0026rsquo;s not just \u0026ldquo;chat with data.\u0026rdquo;\nIt\u0026rsquo;s ask → analyze → act.\nThese agents are typically powered by large language models (LLMs) like GPT-4, paired with:\nRAG (Retrieval-Augmented Generation) — combines a large language model with a search step that fetches relevant facts or data before answering, so responses are grounded in actual campaign performance instead of guesswork. Agent frameworks like LangGraph or CrewAI Orchestration layers to automate repeatable workflows 📉 Dashboards Are Passive. Agents Are Active. Dashboards used to be the gold standard.\nBut now they\u0026rsquo;re often:\nReactive Hard to interpret without context Ignored by clients and decision-makers Analytics agents flip this script.\nTraditional Reporting Analytics Agents You dig through dashboards You ask a question You export to PowerPoint The agent writes the summary You chase performance The agent flags anomalies You run pivot tables The agent suggests reallocation ⚙️ How It Works (Behind the Scenes) User asks a question\ne.g., \u0026ldquo;What was our best-performing channel for Gen Z last month?\u0026rdquo;\nThe agent responds with context-aware analysis\nPulls data from Sheets, BigQuery, dashboards Applies logic (benchmarks, filters, goals) Writes a clear response Delivery via Slack, Notion, or email\nYou can even create multi-agent chains:\nAgent A validates UTM structure Agent B summarizes results Agent C recommends a budget shift 🚀 Real-World Use Cases Media teams are already running agents like these:\n1. 🧾 Report Summary Agent Automatically writes weekly performance summaries for client decks.\n2. 📉 Pacing Monitor Agent Flags under-spending or overspending campaigns in real-time.\n3. ❓ Slack Q\u0026amp;A Agent Answers \u0026ldquo;What\u0026rsquo;s our average CPM?\u0026rdquo; using live campaign data.\n4. 🔍 QA Agent Scans Google Sheets or dashboards for missing naming conventions, flighting gaps, and misaligned UTMs.\n5. 💡 Optimization Agent Suggests reallocations based on ROI curves or historical performance.\n⚖️ Should You Trust Agents Yet? Yes — with guardrails.\nStart with:\nReporting QA checks Summarization Keep strategy decisions and final pacing changes human-in-the-loop.\nThink of agents as:\n\u0026ldquo;Very smart interns with perfect memory and zero ego — but sometimes creative hallucinations.\u0026rdquo;\n🧠 Final Thought Analytics agents aren\u0026rsquo;t replacing analysts.\nThey\u0026rsquo;re replacing the repetitive, soul-sucking parts of analytics —\nSo you can focus on insight, creativity, and strategic impact.\nDashboards gave us access.\nAgents give us action.\n","permalink":"http://www.stencilled.me/posts/2025-04-05-analytics-agents/","summary":"Why Every Media Team Needs an Analytics Agent From dashboards to decisions — AI is now the analyst.\n🧠 The \u0026ldquo;What\u0026rsquo;s Our Best Channel?\u0026rdquo; Moment Imagine this:\nA strategist opens Slack on Monday morning and types:\n\u0026ldquo;What\u0026rsquo;s our best-performing channel in Q2 for CPL under $20?\u0026rdquo;\nSeconds later, the reply pops up:\n🏆 Top channel: Meta (CPL: $18.67, CTR: 1.3%) ⚠️ TikTok pacing 22% behind plan ✅ Google leads in conversion rate (6.","title":"Why Every Media Team Needs an Analytics Agent"},{"content":"The Death of Manual Data Analysis? Why AI Won\u0026rsquo;t Replace You (Yet) Everyone\u0026rsquo;s talking about how AI is changing analytics. Dashboards whisper insights. ChatGPT writes SQL. Some tools promise \u0026ldquo;one-click\u0026rdquo; data stories.\nSo is this the end of manual analysis?\nNot exactly. In fact, it might be the beginning of something better.\nWhat AI Can Do (Really Well) Let\u0026rsquo;s give it credit. In day-to-day analytics work, LLMs and AI tools are already helpful:\n⚡ Generate SQL queries from a prompt 🧠 Summarize dashboard insights in plain English 📈 Detect outliers and spikes in metrics 🔁 Automate recurring reports and QA tasks I\u0026rsquo;ve built internal tools where an analyst types:\n\u0026ldquo;Show me CPC trends for Google Search last month\u0026rdquo;\nAnd the app:\nGenerates SQL Queries BigQuery Returns a chart + insight Flags anything weird via Slack That\u0026rsquo;s not replacing analysts. That\u0026rsquo;s giving them superpowers.\nWhere AI Still Struggles But let\u0026rsquo;s not pretend it\u0026rsquo;s perfect. Here\u0026rsquo;s where LLMs often stumble:\n❌ Mixing up similarly named columns (cost vs cost_per_conversion) ❌ Misreading pivoted data as trends ❌ Suggesting fixes without understanding business rules ❌ Confusing filter logic and slicing the wrong segments Real example: A GPT-based assistant once told me \u0026ldquo;CTR increased 400%.\u0026rdquo;\nTurns out, it was reading the wrong ad group. Impressive confidence. Wrong context.\nThat\u0026rsquo;s why human review still matters.\nWhy Analysts Still Matter Tools are getting smarter. But they can\u0026rsquo;t:\nUnderstand the why behind the data Spot when something just feels \u0026ldquo;off\u0026rdquo; Explain context to stakeholders Design A/B tests and decide tradeoffs Balance growth goals vs media efficiency The analyst role is evolving—not disappearing.\nThink of AI as Your Copilot The best way to approach this shift?\nTreat AI like your intern who works 24/7.\nYou ask the right question It handles the grunt work You validate and present the answer If you\u0026rsquo;re not using AI to do the first pass, you\u0026rsquo;re working harder than you need to.\nTL;DR ✅ AI can automate manual data tasks 🚫 AI can\u0026rsquo;t replace human judgment 🧠 Your value lies in insight, not in clicking filters ⚙️ Learn how to delegate to AI—not fear it ","permalink":"http://www.stencilled.me/posts/2025-02-26-ai-wont-replace-analysts/","summary":"The Death of Manual Data Analysis? Why AI Won\u0026rsquo;t Replace You (Yet) Everyone\u0026rsquo;s talking about how AI is changing analytics. Dashboards whisper insights. ChatGPT writes SQL. Some tools promise \u0026ldquo;one-click\u0026rdquo; data stories.\nSo is this the end of manual analysis?\nNot exactly. In fact, it might be the beginning of something better.\nWhat AI Can Do (Really Well) Let\u0026rsquo;s give it credit. In day-to-day analytics work, LLMs and AI tools are already helpful:","title":"The Death of Manual Data Analysis? Why AI Won't Replace You (Yet)"},{"content":"SENTHIL THYAGARAJAN (504) 908 0008 | senthil.thyagarajan@gmail.com Portfolio: http://www.stencilled.me https://www.linkedin.com/in/senthilthyagarajan/\nPROFESSIONAL EXPERIENCE Analytics Director, Mekanism, New York, NY June 2023 –Present\nBuilt real-time reporting dashboards with Supermetrics \u0026amp; BigQuery, integrating cross-channel data to improve decision-making. Developed a RAG tool that cut insight generation time by 30%, streamlining reporting and enhancing data-driven decision-making. Condensed 100+ slides into a one-page executive report, simplifying media insights for C-suite stakeholders. Embedded a data-driven culture, mentoring teams on analytics tools and integrating data into agency strategy. Created an in-house benchmarking tool, saving $150K annually and improving media performance evaluation. Implemented real-time anomaly detection via Slack/email alerts, reducing response times by 40% and enabling faster optimizations. Drove $2M+ in new client revenue, integrating advanced reporting frameworks into business development. Led and mentored a team of 10, delivering 15+ analytics projects on time, strengthening data-driven decision-making. Associate Director Analytics, Mekanism, New York, NY April 2021 –June 2023\nLed the development of client-specific measurement frameworks, aligning reporting to client goals and ensuring that analytics provided a competitive advantage. Automated reporting pipelines, reducing manual effort by 50% and saving 20+ hours weekly, enabling the team to focus on strategic analysis and client presentations. Drove customer acquisition cost reductions through advanced statistical models, optimizing campaign performance for better ROI. Sr Analytics Manager, Mekanism, New York, NY June 2019–April 2021\nBuilt custom dashboards for real-time media performance tracking, consolidating data from various platforms and simplifying the visualization of key metrics for internal teams. Led sales performance analysis through data modeling and historical forecasting, helping clients link campaign results to business outcomes. Implemented automated workflows for media reporting, reducing manual data extraction and processing time, leading to faster turnaround on key insights. Worked with media planners to refine campaign strategies based on data-driven insights, helping improve media flighting and budget allocation decisions. Analytics Manager, In4mation Insights, New York, NY Feb 2018 –May 2019\nDeveloped and maintained R Shiny applications using Teradata and SQL Server for brand forecasting, evaluation of advertisement, optimization model outputs and pricing activities for CPG and media clients. Guided the client teams with User Acceptance Testing and best practices for using the analytics dashboards thereby onboarding 7 new clients to the new analytics dashboards. Senior GIS Developer (Data Analytics), Tango Analytics – Irving, TX June 2013 –Jan 2018\nDeveloped dashboards using D3js, Python and R for visualizing demographic variables within store trade areas assisting the clients in understanding the underlying important demographics and target marketing. Implemented PL/SQL procedures for generating demographic reports, trade areas, sales forecast model. SKILLS Data Analytics: R Shiny, Looker, Alteryx, Tableau, Origami Logic, AWS, MS Excel\nMedia Analytics: Google Analytics, GTM, AdWords, DCM, Brand lift studies\nMachine Learning: Linear \u0026amp; Logistic Regression, Decision Trees, Random Forests, Gradient Boosting, Causal Analysis, Clustering, Isolation Forests\nLanguages: R, Python, JavaScript, SQL, Google app scripts\nLibraries: R Shiny, d3js, dplyr, tidyR, ggplot2, leaflet, Plotly, Pandas, Numpy, Scikit-learn\nWeb Design: HTML, CSS, Bootstrap, Angular Material Design\nGIS Tools: ArcGIS Suite, QGIS, Map Server, ERDAS Imagine, Google Earth, MapInfo, Maptitude\nEDUCATION University of Texas, Dallas M.S – Geospatial Information Science\nSymbiosis, India M.S – Geoinformatics\nAnna University, India B.S – Electronics and Instrumentation Engineering\nDownload Resume PDF\n","permalink":"http://www.stencilled.me/page/resume/","summary":"SENTHIL THYAGARAJAN (504) 908 0008 | senthil.thyagarajan@gmail.com Portfolio: http://www.stencilled.me https://www.linkedin.com/in/senthilthyagarajan/\nPROFESSIONAL EXPERIENCE Analytics Director, Mekanism, New York, NY June 2023 –Present\nBuilt real-time reporting dashboards with Supermetrics \u0026amp; BigQuery, integrating cross-channel data to improve decision-making. Developed a RAG tool that cut insight generation time by 30%, streamlining reporting and enhancing data-driven decision-making. Condensed 100+ slides into a one-page executive report, simplifying media insights for C-suite stakeholders. Embedded a data-driven culture, mentoring teams on analytics tools and integrating data into agency strategy.","title":"Resume"},{"content":"SENTHIL THYAGARAJAN PROFESSIONAL EXPERIENCE Analytics Director, Mekanism, New York, NY June 2023 –Present\nBuilt real-time reporting dashboards with Supermetrics \u0026amp; BigQuery, integrating cross-channel data to improve decision-making. Developed a RAG tool that cut insight generation time by 30%, streamlining reporting and enhancing data-driven decision-making. Condensed 100+ slides into a one-page executive report, simplifying media insights for C-suite stakeholders. Embedded a data-driven culture, mentoring teams on analytics tools and integrating data into agency strategy. Created an in-house benchmarking tool, saving $150K annually and improving media performance evaluation. Implemented real-time anomaly detection via Slack/email alerts, reducing response times by 40% and enabling faster optimizations. Drove $2M+ in new client revenue, integrating advanced reporting frameworks into business development. Led and mentored a team of 10, delivering 15+ analytics projects on time, strengthening data-driven decision-making. Associate Director Analytics, Mekanism, New York, NY April 2021 –June 2023\nLed the development of client-specific measurement frameworks, aligning reporting to client goals and ensuring that analytics provided a competitive advantage. Automated reporting pipelines, reducing manual effort by 50% and saving 20+ hours weekly, enabling the team to focus on strategic analysis and client presentations. Drove customer acquisition cost reductions through advanced statistical models, optimizing campaign performance for better ROI. Sr Analytics Manager, Mekanism, New York, NY June 2019–April 2021\nBuilt custom dashboards for real-time media performance tracking, consolidating data from various platforms and simplifying the visualization of key metrics for internal teams. Led sales performance analysis through data modeling and historical forecasting, helping clients link campaign results to business outcomes. Implemented automated workflows for media reporting, reducing manual data extraction and processing time, leading to faster turnaround on key insights. Worked with media planners to refine campaign strategies based on data-driven insights, helping improve media flighting and budget allocation decisions. Analytics Manager, In4mation Insights, New York, NY Feb 2018 –May 2019\nDeveloped and maintained R Shiny applications using Teradata and SQL Server for brand forecasting, evaluation of advertisement, optimization model outputs and pricing activities for CPG and media clients. Guided the client teams with User Acceptance Testing and best practices for using the analytics dashboards thereby onboarding 7 new clients to the new analytics dashboards. Senior GIS Developer (Data Analytics), Tango Analytics – Irving, TX June 2013 –Jan 2018\nDeveloped dashboards using D3js, Python and R for visualizing demographic variables within store trade areas assisting the clients in understanding the underlying important demographics and target marketing. Implemented PL/SQL procedures for generating demographic reports, trade areas, sales forecast model. SKILLS Data Analytics: R Shiny, Looker, Alteryx, Tableau, Origami Logic, AWS, MS Excel\nMedia Analytics: Google Analytics, GTM, AdWords, DCM, Brand lift studies\nMachine Learning: Linear \u0026amp; Logistic Regression, Decision Trees, Random Forests, Gradient Boosting, Causal Analysis, Clustering, Isolation Forests\nLanguages: R, Python, JavaScript, SQL, Google app scripts\nLibraries: R Shiny, d3js, dplyr, tidyR, ggplot2, leaflet, Plotly, Pandas, Numpy, Scikit-learn\nWeb Design: HTML, CSS, Bootstrap, Angular Material Design\nGIS Tools: ArcGIS Suite, QGIS, Map Server, ERDAS Imagine, Google Earth, MapInfo, Maptitude\nEDUCATION University of Texas, Dallas M.S – Geospatial Information Science\nSymbiosis, India M.S – Geoinformatics\nAnna University, India B.S – Electronics and Instrumentation Engineering\nDownload Resume PDF\n","permalink":"http://www.stencilled.me/resume/","summary":"SENTHIL THYAGARAJAN PROFESSIONAL EXPERIENCE Analytics Director, Mekanism, New York, NY June 2023 –Present\nBuilt real-time reporting dashboards with Supermetrics \u0026amp; BigQuery, integrating cross-channel data to improve decision-making. Developed a RAG tool that cut insight generation time by 30%, streamlining reporting and enhancing data-driven decision-making. Condensed 100+ slides into a one-page executive report, simplifying media insights for C-suite stakeholders. Embedded a data-driven culture, mentoring teams on analytics tools and integrating data into agency strategy.","title":"Resume"},{"content":" ","permalink":"http://www.stencilled.me/ami-dashboard/","summary":" ","title":"AMI Dashboard"},{"content":"In today\u0026rsquo;s fast-paced media landscape, interactive dashboards have become essential tools for transforming data into actionable insights, ultimately driving media campaign success. Designing a dashboard that offers real-time updates and meaningful insights can be challenging, but with the right approach, it can significantly enhance decision-making and campaign outcomes. Here are some expert tips to help you create interactive dashboards that truly make an impact.\nDefine Clear Objectives Before diving into the design, it\u0026rsquo;s crucial to identify the key objectives of your dashboard. What are the most important metrics for your media campaigns? Aligning your dashboard with these goals ensures it provides relevant and actionable insights. Clear objectives serve as a foundation for a focused and purposeful dashboard.\nUser-Centric Design Understanding your audience is the cornerstone of effective dashboard design. Tailor the dashboard\u0026rsquo;s layout and functionality to the needs of its primary users, whether they are executives, campaign managers, or analysts. A user-centric approach ensures the dashboard delivers the right information to the right people in an accessible manner.\nIntuitive Navigation A well-designed dashboard should be easy to navigate. Use a logical layout to group related metrics together and incorporate clear labels, icons, and tooltips to guide users. An intuitive interface enhances the user experience, making it easier for users to find and understand the data they need.\nReal-Time Data Integration To keep your dashboard up-to-date, implement real-time data feeds. Utilize APIs and data connectors to pull in live data from various sources, ensuring users always have the latest information. Real-time integration is vital for making timely and informed decisions.\nCustomizable Views Allow users to customize their views according to their preferences. Features like selecting date ranges, filtering data by specific criteria, or choosing which metrics to display enhance the dashboard\u0026rsquo;s relevance to individual users. Customizability ensures the dashboard meets the unique needs of its diverse audience.\nInteractive Visualizations Engage users with interactive charts and graphs that facilitate data exploration. Features such as hover-over details, drill-down capabilities, and clickable elements allow users to delve deeper into the data. Interactive visualizations make complex data more accessible and understandable.\nHighlight Key Metrics Emphasize critical KPIs using visual cues like colors, icons, or alerts. Highlighting these metrics ensures they stand out and grab users\u0026rsquo; attention immediately. Key metrics should be easy to spot and interpret at a glance.\nConsistent and Clean Design Maintain a consistent design language throughout the dashboard. A clean and uncluttered layout, with ample white space, improves readability and focus. Consistency in design helps users quickly adapt to and navigate the dashboard.\nMobile Responsiveness Design dashboards that are mobile-friendly. Media professionals often need access to data on the go, so ensure the dashboard is responsive and functions well on various devices. Mobile responsiveness extends the dashboard\u0026rsquo;s usability beyond the desktop.\nPerformance Optimization Ensure the dashboard loads quickly and performs efficiently. Optimize data queries and use efficient coding practices to avoid delays and provide a seamless user experience. Performance is key to user satisfaction and ongoing engagement.\nStorytelling with Data Incorporate storytelling elements to present data in a narrative format. Use annotations, trend lines, and contextual information to help users understand the story behind the numbers. Storytelling transforms raw data into compelling insights.\nRegular Updates and Feedback Loop Continuously improve the dashboard based on user feedback. Regularly update it to include new features or adjust existing ones to better meet user needs. An iterative approach ensures the dashboard evolves with changing requirements and user expectations.\nBy focusing on these tips, you can design interactive dashboards that provide real-time updates and actionable insights, driving the success of your media campaigns. Remember, a well-designed dashboard is more than just a collection of charts and graphs—it\u0026rsquo;s a powerful tool that empowers users to make informed decisions and achieve their campaign goals.\n","permalink":"http://www.stencilled.me/posts/empowering-media-campaigns/","summary":"In today\u0026rsquo;s fast-paced media landscape, interactive dashboards have become essential tools for transforming data into actionable insights, ultimately driving media campaign success. Designing a dashboard that offers real-time updates and meaningful insights can be challenging, but with the right approach, it can significantly enhance decision-making and campaign outcomes. Here are some expert tips to help you create interactive dashboards that truly make an impact.\nDefine Clear Objectives Before diving into the design, it\u0026rsquo;s crucial to identify the key objectives of your dashboard.","title":"Empowering Media Campaigns with Dynamic Analytics"},{"content":"In the realm of digital marketing and media campaigns, establishing a robust data infrastructure is crucial. There\u0026rsquo;s a common misconception that the role of an analytics team is primarily about cleaning data. However, their real value lies in building a solid data infrastructure using ETL (Extract, Transform, Load) tools like Funnel or Supermetrics, and warehousing this data in solutions such as BigQuery. This foundational work ensures that when client onboarding begins, the process is seamless and efficient, allowing campaigns to go live with accurate and actionable reporting.\nThe process starts with the careful extraction of data from various sources, transforming it to meet consistency and quality standards, and then loading it into a centralized data warehouse. Tools like Funnel and Supermetrics simplify this process by automating data collection from numerous marketing platforms, while BigQuery provides a powerful, scalable environment for storing and querying large datasets. Investing time in building this infrastructure upfront saves countless hours during client onboarding and campaign execution. This setup accelerates the reporting process and ensures that the data is reliable and ready for in-depth analysis.\nA critical aspect of this ecosystem involves maintaining strict data standards. Ensuring adherence to naming conventions and data integrity is essential. This attention to detail helps catch errors early, preventing issues down the line and maintaining high data quality. The work done in establishing and maintaining this infrastructure is not merely about data cleaning but upholding the standards that make reliable, insightful reporting possible. This diligent process guarantees that when the time comes to generate reports and dashboards, the data is clean, consistent, and ready to drive strategic decisions.\nUnderstanding the importance of this process is essential for achieving success in digital marketing and media campaigns. A well-built data infrastructure enables faster, more accurate insights, empowering teams to make informed decisions and optimize their campaigns effectively. Recognizing the role of the analytics team as vital gatekeepers of data quality allows organizations to appreciate the strategic value it brings to the table.\n","permalink":"http://www.stencilled.me/posts/building-a-strong-data-infrastructure/","summary":"In the realm of digital marketing and media campaigns, establishing a robust data infrastructure is crucial. There\u0026rsquo;s a common misconception that the role of an analytics team is primarily about cleaning data. However, their real value lies in building a solid data infrastructure using ETL (Extract, Transform, Load) tools like Funnel or Supermetrics, and warehousing this data in solutions such as BigQuery. This foundational work ensures that when client onboarding begins, the process is seamless and efficient, allowing campaigns to go live with accurate and actionable reporting.","title":"Building a Strong Data Infrastructure"},{"content":"So you are looking at numbers everyday. These can be in excel sheets / google sheets while cleaning data or trying to get some insights out of the tables in various dashboards you see daily. These numbers might be currency , rates or just volume of any metrics you want to track.\nSo how would you find the outliers across these tables, the best peforming KPI\u0026rsquo;s , correlations between various metrics. Just numbers in these tables wouldnt help the user in identifying these underlying relationships.Below are a few ways you can do these in google sheets or tables using R.\nUsing R We can also do a similar kind of visualization for tables with numbers using mutiple options. A few of them would be using libraries like DT,Reactable and pivot table js.\nDT This is one of the first packages I started using while trying to publish tables in R shiny apps.The plugins and the extensions in this library makes it easy to copy, filter and searchd data in the tables easy.This package has multiple options to format the values in tables , single rows, bar chart formatting and color scale formatting. Below are the examples of how each formatting option looks like.\nFor more details regarding how to visualize the tables using DT you can read here\nReactable Reactable is one of the recent packages in R which I would say have more functionalites than DT above. Apart from conditonally styling the values in the tables it also allows integrations of html widgets which can render sparkline and barcharts in columns.\nThis first example below shows on how to visualize two columns combined together to understand not only which twitter handle has the most users but also who have the most exclusive followers. The visualization makes the top twitter handles standout.\nThe second table was Women\u0026rsquo;s soccer world cup prediction which was posted on FiveThirtyEight . This clearly shows how the columns are grouped together and visualized to get a better understanding on how each team does in terms of offense and defense. These columns are colored based off the scores. The knockout stages columns for each team use gradual scaled colors which makes it easy fo the readers to understand which team will progress in the tournament.\nrpivotTable This R library is based of open source jspivottable. This package has many functionalites such as rendering charts and pivots. In this post we would look at how this allows user to create heatmaps on table . these can be rendered by rows or columns.\nGoogle Sheets In google sheets you can do it using the option, conditional formatting . This can be done using single colors or color scale.\nSingle color option can be used to format the range you select. This option allows the users to select the colors , add any formulaes like greater than or less than ,vlookups to format the selected range.\nThe second option would be to using the color scales. This option can be used when you also want to visually distinguish the difference between the values in the selected range. For example, a score of 90 would be a shade of green, 60 would a shade of orange and 30 would be a shade of red.\n","permalink":"http://www.stencilled.me/posts/2020-08-15-datafatigue/","summary":"So you are looking at numbers everyday. These can be in excel sheets / google sheets while cleaning data or trying to get some insights out of the tables in various dashboards you see daily. These numbers might be currency , rates or just volume of any metrics you want to track.\nSo how would you find the outliers across these tables, the best peforming KPI\u0026rsquo;s , correlations between various metrics.","title":"Get over that data fatigue"},{"content":"This R shiny app is for all the home chefs out there looking to try different recipes, especially now that we are all social distancing and looking at ways to keep us sane! (Yes, cooking is a form of meditation wherein the end, you have something to eat :D). From a database of 28000 + recipes, just enter the ingredients you wish to filter on and the type of cuisine and you will see all the recipes with your specified ingredients from your favorite cuisine!\nBon Appetite !!\nWhat\u0026rsquo;s Cooking\nYou can find code for the app here.\n","permalink":"http://www.stencilled.me/posts/2020-03-29-whatscooking/","summary":"This R shiny app is for all the home chefs out there looking to try different recipes, especially now that we are all social distancing and looking at ways to keep us sane! (Yes, cooking is a form of meditation wherein the end, you have something to eat :D). From a database of 28000 + recipes, just enter the ingredients you wish to filter on and the type of cuisine and you will see all the recipes with your specified ingredients from your favorite cuisine!","title":"Whats Cooking ??"},{"content":"Most of the shiny apps have tables as the primary component. Now lets say you want to prettify your app and style the tables. All you need understand how tables are built using HTML. This is how the default datatable looks like in the app.\nIn order to build the html table I have used a function table_frame which can be used as a container in DT::renderdatatable. This function basically uses htmltools. For more references on the basics of html tables please refer here\ntable_frame \u0026lt;- function() { htmltools::withTags(table(class = \u0026#39;display\u0026#39;, thead( tr( th(rowspan = 2, \u0026#39;Latitude\u0026#39;), th(rowspan = 2, \u0026#39;Longitude\u0026#39;), th(rowspan = 2, \u0026#39;Month\u0026#39;), th(rowspan = 2, \u0026#39;Year\u0026#39;), th(class = \u0026#39;dt-center\u0026#39;, colspan = 3, \u0026#39;Cloud\u0026#39;), th(rowspan = 2, \u0026#39;Ozone\u0026#39;), th(rowspan = 2, \u0026#39;Pressure\u0026#39;), th(rowspan = 2, \u0026#39;Surface Temperature\u0026#39;), th(rowspan = 2, \u0026#39;Temperature\u0026#39;), tr(lapply(rep( c(\u0026#39;High\u0026#39;, \u0026#39;Low\u0026#39;, \u0026#39;Mid\u0026#39;), 1 ), th)) ) ))) } Tables might have n number of records and its not feasible to display them at once on dashboards. But someone might need to see them all at once. So in tableoptions where we can add two buttons show more and show less. Show less will use the default option of 10 records and show more will display all the records.\ntable_options \u0026lt;- function() { list( dom = \u0026#39;Bfrtip\u0026#39;, #Bfrtip pageLength = 10, buttons = list( c(\u0026#39;copy\u0026#39;, \u0026#39;csv\u0026#39;, \u0026#39;excel\u0026#39;, \u0026#39;pdf\u0026#39;, \u0026#39;print\u0026#39;), list( extend = \u0026#34;collection\u0026#34;, text = \u0026#39;Show All\u0026#39;, action = DT::JS( \u0026#34;function ( e, dt, node, config ) { dt.page.len(-1); dt.ajax.reload();}\u0026#34; ) ), list( extend = \u0026#34;collection\u0026#34;, text = \u0026#39;Show Less\u0026#39;, action = DT::JS( \u0026#34;function ( e, dt, node, config ) { dt.page.len(10); dt.ajax.reload();}\u0026#34; ) ) ), deferRender = TRUE, lengthMenu = list(c(10, 20,-1), c(\u0026#39;10\u0026#39;, \u0026#39;20\u0026#39;, \u0026#39;All\u0026#39;)), searching = FALSE, editable = TRUE, scroller = TRUE, lengthChange = FALSE , initComplete = JS( \u0026#34;function(settings, json) {\u0026#34;, \u0026#34;$(this.api().table().header()).css({\u0026#39;background-color\u0026#39;: \u0026#39;#517fb9\u0026#39;, \u0026#39;color\u0026#39;: \u0026#39;#fff\u0026#39;});\u0026#34;, \u0026#34;}\u0026#34; ) ) } Below is the output how the datatable looks like once the html container and table options are used.So by stying not only can we change the column names but also group them. If you see the default table how we have three columns with prefix cloud. These can be grouped under one column name Cloud.\nCode You can find code for the app here.\n","permalink":"http://www.stencilled.me/posts/2019-04-20-stylingdt/","summary":"Most of the shiny apps have tables as the primary component. Now lets say you want to prettify your app and style the tables. All you need understand how tables are built using HTML. This is how the default datatable looks like in the app.\nIn order to build the html table I have used a function table_frame which can be used as a container in DT::renderdatatable. This function basically uses htmltools.","title":"Styling DataTables"},{"content":"Tables are very much the standard way of representing data in dashboard along with visualizations. Wouldnt it be more useful if you could edit the values in the tables to trigger some calculations and update the values on the fly . These can be used for adjusting allocations or budgets in a project.\nLibraries The libraries which we will be using are shiny for the app itself, dplyr and DT for displaying and editing the tables.\nlibrary(shiny) library(dplyr) library(DT) Data For demo purpose we are creating a dataframe with three brands and few values.\ninput_data \u0026lt;- data.frame(Brand = c(\u0026#34;Brand1\u0026#34;, \u0026#34;Brand2\u0026#34;,\u0026#34;Brand3\u0026#34;), ratio = c (.5, .5, .5), cost = c(2000, 3000, 4000), stringsAsFactors = FALSE) %\u0026gt;% mutate(updated_price = cost * ratio) Editing function/module One thing I have understood over time while building shiny apps is the importance of modules. As the functionalities in the app keep on increasing it becomes difficult to maintain the app in server.r. So I have tried to write the editing function in a module modFunction . You can call this module from the server function of the app.\nmodFunction \u0026lt;- function(input, output, session, data,reset) { v \u0026lt;- reactiveValues(data = data) proxy = dataTableProxy(\u0026#34;mod_table\u0026#34;) observeEvent(input$mod_table_cell_edit, { print(names(v$data)) info = input$mod_table_cell_edit str(info) i = info$row j = info$col k = info$value str(info) isolate( if (j %in% match(c(\u0026#34;ratio\u0026#34;,\u0026#34;cost\u0026#34;,\u0026#34;updated_price\u0026#34;), names(v$data))) { print(match(c(\u0026#34;ratio\u0026#34;,\u0026#34;cost\u0026#34;, \u0026#34;updated_price\u0026#34;), names(v$data))) v$data[i, j] \u0026lt;\u0026lt;- DT::coerceValue(k, v$data[i, j]) print(v$data) if (j %in% match(\u0026#34;cost\u0026#34;, names(v$data))) { v$data$updated_price \u0026lt;\u0026lt;- v$data$cost * v$data$ratio } if (j %in% match(\u0026#34;ratio\u0026#34;, names(v$data))) { v$data$updated_price \u0026lt;\u0026lt;- v$data$cost * v$data$ratio } } else { stop(\u0026#34;You cannot change this column.\u0026#34;) # check to stop the user from editing only few columns } ) replaceData(proxy, v$data, resetPaging = FALSE) # replaces data displayed by the updated table }) ### Reset Table observeEvent(reset(), { v$data \u0026lt;- data # your default data }) print(isolate(colnames(v$data))) output$mod_table \u0026lt;- DT::renderDataTable({ DT::datatable(v$data, editable = TRUE) }) } So if you see the code above once the user edits the ratio or cost it updates the updated_price . You can also allow the user to modify only a few columns as you see above. If the user tries to edit Brand it would throw an error.If the user feels the values in the tables once they are modified doesnt seem to be correct they can click on the reset to get the values in the table to default.\nBelow is the actual code for the shiny app which calls the edit table function modFunction. One other advantage of using the modules it decreases the code to be maintained in the shiny app itself.\nshinyApp( ui = basicPage( mainPanel( actionButton(\u0026#34;reset\u0026#34;, \u0026#34;Reset\u0026#34;), tags$hr(), modFunctionUI(\u0026#34;editable\u0026#34;) ) ), server = function(input, output) { demodata\u0026lt;-input_data callModule(modFunction,\u0026#34;editable\u0026#34;, demodata, reset = reactive(input$reset)) } ) Code You can find code for the app here.\n","permalink":"http://www.stencilled.me/posts/2019-04-18-editable/","summary":"Tables are very much the standard way of representing data in dashboard along with visualizations. Wouldnt it be more useful if you could edit the values in the tables to trigger some calculations and update the values on the fly . These can be used for adjusting allocations or budgets in a project.\nLibraries The libraries which we will be using are shiny for the app itself, dplyr and DT for displaying and editing the tables.","title":"Edit datatables in R shiny app"},{"content":"Most of the dashboards / R shiny app are viewed by the people who want to view the underyling data without doing much analysis. While they view the KPI\u0026rsquo;s , the charts and the tables it would be also an important for them to take the results out of the app for meetings and presenations. One of way doing it is the traditionally downloading the reports. But to make it one step easier they could also be given an option to email these reports.Thats one way of making most out of such tools.\nLibraries The libraries which we will be using are shiny for the app itself, shinyFiles and fs for selecting the files to be mailed and shinyAce and mailR for the email component of the app.\nlibrary(shiny) library(shinyFiles) library(fs) library(shinyAce) library(mailR) The app layout has a sidebar and a main panel. The sidebar has the download and the email functionality and the main panel has the plot . The user can donload the the report in mutiple formats.\nDownload Reports Below is the code for downloading report. The user can download the reports in multiple formats. You could also write into powerpoints which can taken into weekly meetings.\noutput$downloadReport \u0026lt;- downloadHandler( filename = function() { paste(\u0026#39;my-report\u0026#39;, sep = \u0026#39;.\u0026#39;, switch( input$format, PDF = \u0026#39;pdf\u0026#39;, HTML = \u0026#39;html\u0026#39;, Word = \u0026#39;docx\u0026#39; )) }, content = function(file) { src \u0026lt;- normalizePath(\u0026#39;report.Rmd\u0026#39;) # temporarily switch to the temp dir, in case you do not have write # permission to the current working directory owd \u0026lt;- setwd(tempdir()) on.exit(setwd(owd)) file.copy(src, \u0026#39;report.Rmd\u0026#39;, overwrite = TRUE) library(rmarkdown) out \u0026lt;- render(\u0026#39;report.Rmd\u0026#39;, switch( input$format, PDF = pdf_document(), HTML = html_document(), Word = word_document() )) file.rename(out, file) } ) Email component The following code handles how to email from the app itself.Once the user downloads the report to their local drive, they can select any files they want to mail to using shinyFiles\nvolumes \u0026lt;- c(Home = fs::path_home(), \u0026#34;R Installation\u0026#34; = R.home(), getVolumes()()) shinyFileChoose(input, \u0026#34;file\u0026#34;, roots = volumes, session = session) When the user clicks on the send button it takes the inputs such as to, the body of the mail and the file to be mailed which was selected by the user.In this app the email is configured via gmail but this can be done via Outlook as well.\nif(is.null(input$send) || input$send==0) return(NULL) subject \u0026lt;- isolate(input$subject) msg \u0026lt;- isolate(input$message) sender \u0026lt;- \u0026#34;from@gmail.com\u0026#34; recipients \u0026lt;- isolate(input$to) path \u0026lt;- parseFilePaths(volumes, input$file) send.mail(from = sender, to = recipients, subject = subject, body = msg, #body, #attach.files = path$datapath,#file.path(folder, fileName), attach.files = path$datapath, smtp = list(host.name = \u0026#34;smtp.gmail.com\u0026#34;, port = 465, user.name=sender, passwd=\u0026#34;*********\u0026#34;, ssl=TRUE), authenticate = TRUE, send = TRUE) }) Code You can find code for the app here.\n","permalink":"http://www.stencilled.me/posts/2019-04-14-downloadshiny/","summary":"Most of the dashboards / R shiny app are viewed by the people who want to view the underyling data without doing much analysis. While they view the KPI\u0026rsquo;s , the charts and the tables it would be also an important for them to take the results out of the app for meetings and presenations. One of way doing it is the traditionally downloading the reports. But to make it one step easier they could also be given an option to email these reports.","title":"Download and Email reports in R Shiny app"},{"content":" Injustice : God Among Us Year one - Tom Taylor 5/5 ","permalink":"http://www.stencilled.me/page/reading/","summary":" Injustice : God Among Us Year one - Tom Taylor 5/5 ","title":""},{"content":"In this post we see how the no. of breweries stack up against no. of colleges across USA. Here I am using d3js and a grid bar chartwhere each grid represents 5 breweries/colleges.I collected the data for Brewery from Beer Advocate and for colleges from wikipedia\n\u003c!DOCTYPE html\u003e More Breweries than Colleges Breweries vs Colleges in US = 5 Top 10 States Bottom 10 States College Brewery \u0026lt;script src=\u0026quot;https://cdnjs.cloudflare.com/ajax/libs/d3/4.4.4/d3.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;https://cdnjs.cloudflare.com/ajax/libs/lodash.js/4.17.11/lodash.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; var tilesPerRow = 5; var tileSize = 15; var barPadding = 20; var maxValue = 100; var numVisibleCountries = 10; var barWidth = (tilesPerRow * tileSize) + barPadding; var data, filteredData; let colors = [\u0026quot;#8dd3c7\u0026quot;,\u0026quot;#a6cee3\u0026quot;,\u0026quot;#bebada\u0026quot;,\u0026quot;#fb8072\u0026quot;,\u0026quot;#80b1d3\u0026quot;,\u0026quot;#fdb462\u0026quot;,\u0026quot;#b3de69\u0026quot;,\u0026quot;#fccde5\u0026quot;,\u0026quot;#d9d9d9\u0026quot;,\u0026quot;#bc80bd\u0026quot;]; var selectedYear = \u0026quot;College\u0026quot;, selectedMode = \u0026quot;top10\u0026quot;; function initializeData() { data = data.map(function(d) { return { name: d.country, year: d.year, age: +d.all } }); } function updateFilteredData() { filteredData = data.filter(function(d) { return d.year === selectedYear; }); filteredData = _.sortBy(filteredData, function(d) { return selectedMode === \u0026quot;top10\u0026quot; ? -d.age : d.age; }); filteredData = filteredData.slice(0, numVisibleCountries); } function getTiles(num) { var tiles = []; for(var i = 0; i \u0026lt; num; i++) { var rowNumber = Math.floor(i / tilesPerRow); tiles.push({ x: (i % tilesPerRow) * tileSize, y: -(rowNumber + 1) * tileSize }); } return tiles } function updateBar(d, i) { var tiles = getTiles(d.age/5); var u = d3.select(this) .attr(\u0026quot;transform\u0026quot;, \u0026quot;translate(\u0026quot; + i * barWidth + \u0026quot;, 300)\u0026quot;) .selectAll(\u0026quot;rect\u0026quot;) .data(tiles); u.enter() .append(\u0026quot;rect\u0026quot;) .style(\u0026quot;opacity\u0026quot;, 0) .style(\u0026quot;stroke\u0026quot;, \u0026quot;white\u0026quot;) .style(\u0026quot;stroke-width\u0026quot;, \u0026quot;0.5\u0026quot;) .style(\u0026quot;shape-rendering\u0026quot;, \u0026quot;crispEdges\u0026quot;) .merge(u) .attr(\u0026quot;x\u0026quot;, function(d) { return d.x; }) .attr(\u0026quot;y\u0026quot;, function(d) { return d.y; }) .attr(\u0026quot;width\u0026quot;, tileSize) .attr(\u0026quot;height\u0026quot;, tileSize) .transition() .delay(function(d, i) { return i * 20; }) .style(\u0026quot;opacity\u0026quot;, 1); u.exit() .transition() .delay(function(d, i) { return (100 - i) * 20; }) .style(\u0026quot;opacity\u0026quot;, 0) .on(\u0026quot;end\u0026quot;, function() { d3.select(this).remove(); }); } function updateLabel(d) { var el = d3.select(this) .select(\u0026quot;text\u0026quot;); if(el.empty()) { el = d3.select(this) .append(\u0026quot;text\u0026quot;) .attr(\u0026quot;y\u0026quot;, -4) .attr(\u0026quot;transform\u0026quot;, \u0026quot;rotate(-90)\u0026quot;) .style(\u0026quot;font-weight\u0026quot;, \u0026quot;bold\u0026quot;) .style(\u0026quot;font-size\u0026quot;, \u0026quot;12px\u0026quot;) .style(\u0026quot;fill\u0026quot;, \u0026quot;#777\u0026quot;); } el.text(d.name); } function updateBars() { var u = d3.select(\u0026quot;g.bars\u0026quot;) .selectAll(\u0026quot;g\u0026quot;) .data(filteredData); u.enter() .append(\u0026quot;g\u0026quot;) .merge(u) .style(\u0026quot;fill\u0026quot;, function(d, i) { return colors[i % colors.length]; }) .each(updateBar) .each(updateLabel); u.exit().remove(); } function updateAxis() { var chartWidth = numVisibleCountries * barWidth; var chartHeight = (maxValue / tilesPerRow) * tileSize; var yScale = d3.scaleLinear().domain([0, maxValue]).range([chartHeight, 0]); var yAxis = d3.axisRight().scale(yScale).tickSize(chartWidth); d3.select(\u0026quot;.y.axis\u0026quot;) .call(yAxis); } function initialize() { initializeData(); d3.select(\u0026quot;select.mode\u0026quot;) .on(\u0026quot;change\u0026quot;, function() { selectedMode = this.value; update(); }) d3.select(\u0026quot;select.year\u0026quot;) .on(\u0026quot;change\u0026quot;, function() { selectedYear = this.value; update(); }); } function update() { updateFilteredData(); updateBars(); updateAxis(); } d3.tsv(\u0026quot;College_Brewery.tsv\u0026quot;, function(err, tsv) { data = tsv; initialize(); update(); }); \u0026lt;/script\u0026gt; ","permalink":"http://www.stencilled.me/posts/2018-09-26-college-brewery/","summary":"In this post we see how the no. of breweries stack up against no. of colleges across USA. Here I am using d3js and a grid bar chartwhere each grid represents 5 breweries/colleges.I collected the data for Brewery from Beer Advocate and for colleges from wikipedia\n\u003c!DOCTYPE html\u003e More Breweries than Colleges Breweries vs Colleges in US = 5 Top 10 States Bottom 10 States College Brewery \u0026lt;script src=\u0026quot;https://cdnjs.cloudflare.com/ajax/libs/d3/4.4.4/d3.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;https://cdnjs.","title":"Colleges vs Breweries - What tops in your State ?"},{"content":"Recently while I was searching for trend data I came across Google Trends. So in the post I am using gtrends, a r package to understand the trends and display it using a R shiny app using the shinymaterial package. The shinymaterial package tends to move away from the traditional shiny dashboard layouts and design. You can use this shiny app from here.\n","permalink":"http://www.stencilled.me/posts/2018-09-24-gtrends/","summary":"Recently while I was searching for trend data I came across Google Trends. So in the post I am using gtrends, a r package to understand the trends and display it using a R shiny app using the shinymaterial package. The shinymaterial package tends to move away from the traditional shiny dashboard layouts and design. You can use this shiny app from here.","title":"Google Trends using gtrends and R shiny"},{"content":"Recently I was searching for some gifts when I stumbled across Sound Viz and Cumberland Coast . These were your favorite songs which were visualized and printed. So that\u0026rsquo;s when I started looking into on how could you visualize them. So I wanted to try the same and that’s when I came across the r packages tuneR and seewave. Now I knew that I wanted to visualize the song but then I also wanted to see how different are the visualizations for the same song in an original version and the acoustic version. So I did download both the versions of the song Take On Me by A-ha. Since the wave format is too big to-be processed I did clip the songs to 1-minute duration. Below are the visualizations for the different versions of the same song.\nFollowing is the code which I used to generate the waveforms above:\nlibrary(tuneR) library(seewave) setWavPlayer(\u0026#39;mplayer\u0026#39;) # Set the command-line WAV player fin \u0026lt;- readWave(\u0026#39;yoyoma_clip.wav\u0026#39;) data = (fin) snd = data@left par(bg=\u0026#34;orange\u0026#34;) # plot waveform orig_left \u0026lt;- plot(snd, type = \u0026#39;l\u0026#39;, xlab = \u0026#39;Samples\u0026#39;, ylab = \u0026#39;Amplitude\u0026#39;,col = c( \u0026#34;blue\u0026#34;,\u0026#34;green\u0026#34;, \u0026#34;orange\u0026#34;),frame.plot=FALSE) And finally finishing this post with one of my favorite song.\n","permalink":"http://www.stencilled.me/posts/2018-10-02-song-wave/","summary":"Recently I was searching for some gifts when I stumbled across Sound Viz and Cumberland Coast . These were your favorite songs which were visualized and printed. So that\u0026rsquo;s when I started looking into on how could you visualize them. So I wanted to try the same and that’s when I came across the r packages tuneR and seewave. Now I knew that I wanted to visualize the song but then I also wanted to see how different are the visualizations for the same song in an original version and the acoustic version.","title":"Visualizing your favourite song?"},{"content":" ","permalink":"http://www.stencilled.me/posts/2018-08-16-mta-subway-ridership/","summary":" ","title":"MTA Subway Ridership"},{"content":"In this day and age of so many sharing services like Uber and Lyft , pricey hotels are being replaced by Airbnb. Students, working people and travelers wouldn\u0026rsquo;t always want to pay a high price for staying a couple of nights at the Marriott and would rather stay at a place where that has the basic amenities needed for them at a reasonable price. In this project I am trying to understand the listings put on Airbnb on how the price varies by neighborhood ,house type and various other factors.\nI am using the data for New York for this post. To start with we can see the properties listed by neighborhood across New York.In the table below you can see the count for the Airbnb listings aggregated at burrough level and neighborhood level.\nBurroughCount Bronx 649 Brooklyn 16810 Manhattan 19212 Queens 3821 Staten Island 261 NeighborhoodCount Allerton 23 Arden Heights 6 Arrochar 14 Arverne 71 Astoria 755 Bath Beach 11 Battery Park City 65 Bay Ridge 91 Bay Terrace 5 Bay Terrace, Staten Island 1 Baychester 6 Bayside 40 Bayswater 8 Bedford-Stuyvesant 2850 Belle Harbor 5 Bellerose 10 Belmont 8 Bensonhurst 44 Bergen Beach 3 Boerum Hill 153 Borough Park 90 Briarwood 28 Brighton Beach 46 Bronxdale 12 Brooklyn Heights 129 Brownsville 42 Bushwick 1937 Cambria Heights 13 Canarsie 75 Carroll Gardens 227 ⋮⋮ Sunset Park 312 Theater District 173 Throgs Neck 8 Todt Hill 2 Tompkinsville 28 Tottenville 3 Tremont 5 Tribeca 156 Two Bridges 55 Unionport 4 University Heights 16 Upper East Side 1543 Upper West Side 1782 Van Nest 10 Vinegar Hill 26 Wakefield 21 Washington Heights 870 West Brighton 11 West Farms 6 West Village 780 Westchester Square 2 Westerleigh 2 Whitestone 5 Williamsbridge 24 Williamsburg 3719 Windsor Terrace 128 Woodhaven 36 Woodlawn 6 Woodrow 2 Woodside 113 ####Summarizing Price\nAs we saw the count for listings at neighborhood and burrough level below are the prices .\nBurroughPricing($) Bronx 83 Brooklyn 120 Manhattan 181 Queens 95 Staten Island129 NeighbourhoodPricing($) Allerton 69 Arden Heights 63 Arrochar 223 Arverne 93 Astoria 99 Bath Beach 106 Battery Park City 221 Bay Ridge 90 Bay Terrace 144 Bay Terrace, Staten Island 75 Baychester 54 Bayside 86 Bayswater 81 Bedford-Stuyvesant 102 Belle Harbor 166 Bellerose 91 Belmont 56 Bensonhurst 81 Bergen Beach 154 Boerum Hill 158 Borough Park 112 Briarwood 130 Brighton Beach 112 Bronxdale 66 Brooklyn Heights 255 Brownsville 72 Bushwick 84 Cambria Heights 75 Canarsie 126 Carroll Gardens 183 ⋮⋮ Sunset Park 106 Theater District 232 Throgs Neck 98 Todt Hill 257 Tompkinsville 69 Tottenville 218 Tremont 62 Tribeca 353 Two Bridges 123 Unionport 65 University Heights 60 Upper East Side 173 Upper West Side 195 Van Nest 170 Vinegar Hill 173 Wakefield 108 Washington Heights 91 West Brighton 77 West Farms 205 West Village 240 Westchester Square 70 Westerleigh 785 Whitestone 148 Williamsbridge 92 Williamsburg 140 Windsor Terrace 129 Woodhaven 59 Woodlawn 69 Woodrow 458 Woodside 83 We could also see the listings per zip code level. I have use Ari Lamstien\u0026rsquo;s R package choroplethrzip . Taking the five burroughs of New York and all the zip codes within them I aggregated the listings per zipcode and mapped them .As you can see Manhattan and Brooklyn regions are the one with most listings.\nThe Airbnb listings are generally categorized as an Entire Apartment / Home , private room and shared room.Below is the pricing for each type of listing.\nRoom TypePrice Entire home/apt207 Private room 88 Shared room 71 This graph below shows how each of the Burroughs have listings by property type.\nTo understand how the listings are spatially located I did plot them and bin them by property time to visualize where the listings are shared or entire apartment.\nSubway vs Rental Listings\nApart from all the amenities mentioned in the listing one of the most important factor when it comes to booking a listing in New York is the proximity to a subway stop. I downloaded a json for the subways and plotted them against the listings. Considering that people want to live close-by I took as 0.1 mile as a walking distance , I created a buffer to capture all the listings inside that 0.1 mile ring.I took the top 20 to understand which of the subway stations had the most listings nearby.\nThe same results were also plotted on the map using the leaflet package to visualize where these subway stations are located and how many listings they have using the graduated symbols.\nDescription\nThe description and the photos put by the property owner plays an important role for anyone to book a listing . A word-cloud of those description helps us understand what do property owners mention in the description which might help them in more bookings.\nAmenties\nWhen you book a listing in Airbnb I assume the first thing we look for is does the listing have wifi /internet . So based on the amenities provided across all listings I mapped a word cloud to see what are the top amenities listed by the owners for the property.\nReviews Once the user goes through the list of amenities they do scroll down through the reviews to see what people who have stayed in this property thought about. Was the property as per mentioned in the listing ? Was the bed making noise ? Any suggestions for restaurants nearby?\nWhat Next\nThe next stage for this project is to identify a relationship between the proximity of listing to subway stations , amenities listed for the property,POI\u0026rsquo;s such as restaurants, workplace, demographic variables such as daytime population. A shiny app which would show the listings and various layers such as rental price by geographies , all in one place for all the cities Airbnb have the rentals listed.\n","permalink":"http://www.stencilled.me/posts/2017-10-25-airbnb-listing/","summary":"In this day and age of so many sharing services like Uber and Lyft , pricey hotels are being replaced by Airbnb. Students, working people and travelers wouldn\u0026rsquo;t always want to pay a high price for staying a couple of nights at the Marriott and would rather stay at a place where that has the basic amenities needed for them at a reasonable price. In this project I am trying to understand the listings put on Airbnb on how the price varies by neighborhood ,house type and various other factors.","title":"Visualizing Airbnb listings."},{"content":"While working on a couple of projects involving beer data I did land up at breweries association. They have a data set on all the breweries across the US. This raw html table was scrapped ,cleaned and geocoded using R. You can find the R Scripts which I used for this project here.\nOnce the data was ready this was put to use by being displayed on a web mapping application built on js and bootstrap. This responsive web application can be used on your mobile devices can be used while you are traveling places or planning to try a few new taps near you . A few other features in this map application are Search functionality using autocomplete ,find 5 nearest breweries from your current location and share via email and filter the breweries that are being displayed in your current extent.\nCheck it out , Taps Near You. CHEERS !!!\n","permalink":"http://www.stencilled.me/posts/2017-09-18-taps-near-you/","summary":"While working on a couple of projects involving beer data I did land up at breweries association. They have a data set on all the breweries across the US. This raw html table was scrapped ,cleaned and geocoded using R. You can find the R Scripts which I used for this project here.\nOnce the data was ready this was put to use by being displayed on a web mapping application built on js and bootstrap.","title":"Taps near you"},{"content":"I started working on this visualation after coming across Mike Bostock\u0026rsquo;s shape tweening bl.ock , which was done for one state. The source for this data is Insurance Institute for Highway Service(IIHS). The size of the square is based on the motor vehicle deaths per 100,000 people(2015).\n\u0026lt;script src=\u0026quot;https://d3js.org/d3.v4.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;polylabel.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; var width = window.innerWidth, height = window.innerHeight, duration =1000; //Set up the colour scale var color = d3.scaleOrdinal(d3.schemeCategory10); var svg = d3.select(\u0026quot;body\u0026quot;).append(\u0026quot;svg\u0026quot;) .attr(\u0026quot;width\u0026quot;, width) .attr(\u0026quot;height\u0026quot;, height); d3.json(\u0026quot;us.json\u0026quot;, function(map) { var projection = centerZoom(map); var polygons = []; map.features.forEach(function(feature){ polygons.push({id: feature.properties.state,count: feature.properties.count , geom: feature.geometry})\t}); var init = parse(polygons, projection).sort(function(a, b){ return b.area - a.area; }); init.forEach(function(d){ var path = drawPath(d).attr(\u0026quot;d\u0026quot;, d.d0) drawLabels(d, 0, 0); }); d3.interval(function(){ var steps = stepUpdate(); stepChange(steps[0], steps[1], init); }, duration * 2); }); // This function \u0026quot;centers\u0026quot; and \u0026quot;zooms\u0026quot; a map by setting its projection's scale and translate according to its outer boundary function centerZoom(data){ // create a first guess for the projection var scale = 1; var offset = [width / 2, height / 2]; var projection = d3.geoAlbersUsa().scale(scale).translate(offset); // get bounds var bounds = d3.geoPath().projection(projection).bounds(data); // calculate the scale and offset var hscale = scale * width / (bounds[1][0] - bounds[0][0]); var vscale = scale * height / (bounds[1][1] - bounds[0][1]); var scale = (hscale \u0026lt; vscale) ? hscale : vscale; var offset = [width - (bounds[0][0] + bounds[1][0]) / 2, height - (bounds[0][1] + bounds[1][1]) / 2]; // new projection projection = d3.geoAlbersUsa() .scale(scale) .translate(offset); return projection; } function drawLabels(obj,oldStep,newStep){ var pOld = polylabel([obj[\u0026quot;coordinates\u0026quot; + oldStep]], 1); var pNew = polylabel([obj[\u0026quot;coordinates\u0026quot; + newStep]], 1); svg.append(\u0026quot;text\u0026quot;) .attr(\u0026quot;class\u0026quot;, \u0026quot;state state-label\u0026quot;) .attr(\u0026quot;x\u0026quot;, pOld[0]) .attr(\u0026quot;y\u0026quot;, pOld[1]) .attr(\u0026quot;dy\u0026quot;, \u0026quot;0.35em\u0026quot;) .attr(\u0026quot;text-anchor\u0026quot;, \u0026quot;middle\u0026quot;) .text(obj.id ) //\t.text(obj.id + \u0026quot;: \u0026quot;+\u0026quot;\\n\u0026quot; + obj.count) .transition().duration(duration) .attr(\u0026quot;x\u0026quot;, pNew[0]) .attr(\u0026quot;y\u0026quot;, pNew[1]) } function drawPath(obj){ var path = svg.append(\u0026quot;path\u0026quot;) .attr(\u0026quot;class\u0026quot;, \u0026quot;state state-path\u0026quot;) .attr(\u0026quot;id\u0026quot;, obj.id) .style(\u0026quot;fill\u0026quot;, function(d) { return color([obj.count]) }) return path; } var obj = {}; function parse(polygons, projection) {\nvar arr = []; polygons.forEach(function(state){ obj = {}; obj.id = state.id; obj.count = state.count; obj.coordinates0 = state.geom.coordinates[0].map(projection); obj.coordinates1 = square(obj.coordinates0)[0]; obj.d0 = \u0026quot;M\u0026quot; + obj.coordinates0.join(\u0026quot;L\u0026quot;) + \u0026quot;Z\u0026quot;; obj.d1 = \u0026quot;M\u0026quot; + obj.coordinates1.join(\u0026quot;L\u0026quot;) + \u0026quot;Z\u0026quot;; obj.area = square(obj.coordinates0)[1]; arr.push(obj); }); return arr; } function square(coordinates){ var area = obj.count * obj.count *20; area \u0026lt; 0 ? area = area * -1 : area = area; var r = Math.sqrt(area) / 2.5; var centroid = d3.polygonCentroid(coordinates); var x = centroid[0]; var y = centroid[1]; var len = coordinates.length; var square = squareCoords(x, y, r, len); return [square, area]; } function squareCoords(x, y, r, len){ var square = []; var topLf = [x - r, y - r]; var topRt = [x + r, y - r]; var botRt = [x + r, y + r]; var botLf = [x - r, y + r]; for (var i = 0; i \u0026lt; len / 4; i++){ square.push(botRt); } for (var i = 0; i \u0026lt; len / 4; i++){ square.push(botLf); } for (var i = 0; i \u0026lt; len / 4; i++){ square.push(topLf); } for (var i = 0; i \u0026lt; len / 4; i++){ square.push(topRt); } return square; } function stepChange(oldStep, newStep, obj){ d3.selectAll(\u0026quot;.state\u0026quot;).remove(); obj.forEach( function(d){ transitionPath(drawPath(d), d[\u0026quot;d\u0026quot; + oldStep], d[\u0026quot;d\u0026quot; + newStep], duration); drawLabels(d, oldStep, newStep); } ); } function stepUpdate(){ var currStep = +d3.select(\u0026quot;body\u0026quot;).attr(\u0026quot;step\u0026quot;), newStep; var newStep = currStep == 0 ? 1 : 0; d3.select(\u0026quot;body\u0026quot;).attr(\u0026quot;step\u0026quot;, newStep); return [currStep, newStep]; } function transitionPath(path, d0, d1, duration){ path .attr(\u0026quot;d\u0026quot;, d0) .transition().duration(duration) .attr(\u0026quot;d\u0026quot;, d1); } \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; ","permalink":"http://www.stencilled.me/posts/2017-05-10-states-to-shapes/","summary":"\u003cp\u003eI started working on this visualation after coming across  \u003ca href=\"https://bl.ocks.org/mbostock/3081153\"\u003eMike Bostock\u0026rsquo;s shape tweening bl.ock\u003c/a\u003e ,\nwhich was done for one state. The source for this data is\nInsurance Institute for Highway Service(IIHS). The size of the square is based on the motor vehicle deaths per 100,000 people(2015).\u003c/p\u003e","title":"States to Shapes"},{"content":" When you think about Texas, the first thing that would come in mind is barbecue which of course pairs well with a good beer. In this post I have mapped each state in the US ranked based on the breweries or wineries present. I got the data from the Brewers Association and the American Winery Guide. With data visualization being so important in representing the dataset I have used hexbins shapefile from Don Meltz\u0026rsquo;s repo.\nBelow are the maps for how the states are ranked . ","permalink":"http://www.stencilled.me/posts/2017-04-29-whats-your-states-poison/","summary":"\u003cstyle\u003e\n\n.myIframe {\nposition: relative;\npadding-bottom: 65.25%;\npadding-top: 30px;\nheight: 0;\noverflow: auto;\n-webkit-overflow-scrolling:touch; //\u003c\u003c--- THIS IS THE KEY\nborder: solid black 1px;\n}\n.myIframe iframe {\nposition: absolute;\ntop: 0;\nleft: 0;\nwidth: 100%;\nheight: 100%;\n}\n\u003c/style\u003e\n\u003cp\u003eWhen you think about Texas, the first thing that would come in mind is barbecue which of course  pairs well with a good beer. In this post I have mapped each state in the\nUS ranked based on the breweries or wineries present. I got the data from the Brewers Association and the American Winery Guide. With data visualization being so\nimportant in representing the dataset I have used hexbins shapefile from   \u003ca href=\"https://github.com/donmeltz/US-States---Hexbins\"\u003eDon Meltz\u0026rsquo;s repo\u003c/a\u003e.\u003c/p\u003e","title":"Breweries or Wineries ??"},{"content":"As I have been using R for a while, one of the things I wanted to do was a time series map. Most of the time series maps I see have sliders to change the years. While looking at how to make time series maps I happened to lear how to make a GIF with a set of images.\nSo I thought, how about making an election based map over the past elections. First I couldn’t get the historical data. Also, there’s an abundance of election news right now anyways, so I didn’t want to take on that topic. Then I went on making several maps to see which demographic variable would be a right fit for this exercise. I ended up using “People with at least one Health Insurance coverage”. This variable was strictly selected for learning purpose. The library I have used here is Choroplethr and ChoroplethrMaps by Ari Lamstien.\nNow to get the demographics data into R I have used the American Community Survey(ACS) library. For this you must get API key from ACS website.\nlibrary(acs) library(choroplethr) library(choroplethrMaps) library(ggplot2) #api.key.install(key=\u0026#34;API from the ACS website\u0026#34;) Once this is done you can use the ACS lookup function to search for the variables list.\nacs.lookup(keyword = \u0026#34;Population\u0026#34;, endyear = 2013 ) Now as I mentioned above I did want to do make maps for over a time period. So their are two ways to make them. Either save each map individually or just put them through a loop for all the years you want to make maps for . Below is the code for how to put through a loop and save all the maps at once.\n# array of years vr\u0026lt;-c(2013,2014,2015) for (i in 1:3) { vr[i] \u0026lt;- vr[i] print(vr[i]) getdata = get_acs_data(\u0026#34;B27010\u0026#34;, \u0026#34;county\u0026#34;,endyear=vr[i],span = 5, include_moe = FALSE) str(getdata) #to get this into a dataframe df = getdata[[1]] data(county.regions) head(county.regions) df2 = merge(df, county.regions) df2 = df2[order(-df2$value), ] head(df2) totalCount \u0026lt;- sum(df2$value) totalCount myplot\u0026lt;- county_choropleth(df2, title = vr[i],legend = totalCount,county_zoom = NULL) #use paste to covert params passed into it to a Character string. ggsave(myplot,filename=paste(\u0026#34;Health Coverage\u0026#34;, vr[i],\u0026#34;.png\u0026#34;,sep=\u0026#34;\u0026#34;)) } So once this was done I get images like the one below for each year I have mentioned in the code. ![This is an image](Health Coverage2013.png) Once this was done I wanted to make a gif for all the individual maps. I have used ImageMagick for it . Once your download and install it go to the command prompt and go the path where all the images have been saved. Below is the command to generate the gifs.\nmagick -delay 100 -loop 0 *.png animation.gif To break it down first you define how much should be the delay time , then select all the pngs in the folder and then the name you want to save the gifs.\nThis particular exercise was mainly on learning how to make choropleth maps using demographics variable and publish them as a GIF to see how the variable varies over a time period. Also you could see in the numbers that the Insurance coverage did increase over the time period.\nPost this I did want to see how it could be done for all the states in the US separately. Below is the code and the output too. The demographics variable I have used here is the CY population.\nlibrary(acs) library(choroplethr) library(choroplethrMaps) library(ggplot2) #api.key.install(key=\u0026#34;API from the ACS website\u0026#34;) vr\u0026lt;-c(2015) for (i in 1:1) { vr[i] \u0026lt;- vr[i] print(vr[i]) l = get_acs_data(\u0026#34;B00001\u0026#34;, \u0026#34;county\u0026#34;,endyear=vr[i],span = 5, include_moe = TRUE) str(l) df = l[[1]] head(df) data(county.regions) head(county.regions) df2 = merge(df, county.regions) df2 = df2[order(-df2$value), ] head(df2) totalCount \u0026lt;- sum(df2$value) totalCount stt \u0026lt;- c(unique(df2$state.name)) stt for ( i in 1:51) { print(stt[i]) myplot\u0026lt;- county_choropleth(df2, title = \u0026#34;\u0026#34;,legend = stt[i],state_zoom = stt[i]) ggsave(myplot,filename=paste(\u0026#34;Population_\u0026#34;,stt[i],\u0026#34;.png\u0026#34;,sep=\u0026#34;\u0026#34;)) } } You could change to any demographics variable you want and see how many years the data is available for and make maps as per your needs. Happy GIFFING !!!\n","permalink":"http://www.stencilled.me/posts/2017-03-20-map-gif-r/","summary":"As I have been using R for a while, one of the things I wanted to do was a time series map. Most of the time series maps I see have sliders to change the years. While looking at how to make time series maps I happened to lear how to make a GIF with a set of images.\nSo I thought, how about making an election based map over the past elections.","title":"Maps and gifs"},{"content":" After completing my previous post on food I wanted to work on something which I have started to explore recently,craft beer. A friend of mine introduced me to a beer club membership prior to which I never knew anything beyond the Corona\u0026rsquo;s . Then began the collection and here it is, what have so far.\nSo when I started searching for the data google lead me to Beer Advocate. Below is how the raw html table from the website looked like.\nI have used R to scrape the table from the website using R. The library I am using here to scrape is Rvest. Below is the code on how to get the data.\nlibrary(\u0026#34;rvest\u0026#34;) # Enter the url below url \u0026lt;- \u0026#34;https://www.beeradvocate.com/lists/top/\u0026#34; beer \u0026lt;- url %\u0026gt;% html() %\u0026gt;% ## to get xpath for a table ,right click on the table,inspect, ## go to the table tag ,right click again and go to copy xpath .. phew ... ## not clear click here for \u0026lt;a href=\u0026#34;http://www.wikihow.com/Find-XPath-Using-Firebug\u0026#34;\u0026gt;more details\u0026lt;/a\u0026gt; html_nodes(xpath = \u0026#39;//*[@id=\u0026#34;extendedInfo\u0026#34;]/a[1]\u0026#39;) %\u0026gt;% html_table() beer \u0026lt;- beer[[1]] head(beer) write.table( beer, file = \u0026#34;topus250.csv\u0026#34;, quote = TRUE, sep = \u0026#34;,\u0026#34;, row.names = FALSE ) Now that I got the scraped data and address parameter as the name of the brewing company , it looks something like this.\nThe next step here is to get the address geocoded which would help me plot this on a map . For this I have used the library ggmap.\nlibrary(\u0026#34;rvest\u0026#34;) library(ggmap) # Read in the CSV data and store it in a variable origAddress \u0026lt;- read.csv(\u0026#34;topus250.csv\u0026#34;, stringsAsFactors = FALSE) # Initialize the data frame geocoded \u0026lt;- data.frame(stringsAsFactors = FALSE) # Loop through the addresses to get the latitude and longitude of each address # and add it to the origAddress data frame in new columns lat and lon for(i in 1:nrow(origAddress)) { # Print(\u0026#34;Working...\u0026#34;) result \u0026lt;- geocode(origAddress$Address[i], output = \u0026#34;latlona\u0026#34;, source = \u0026#34;google\u0026#34;) origAddress$lon[i] \u0026lt;- as.numeric(result[1]) origAddress$lat[i] \u0026lt;- as.numeric(result[2]) origAddress$geoAddress[i] \u0026lt;- as.character(result[3]) origAddress$state[i] \u0026lt;- as.character(result[3]) } # Save the output as csv to the working directory write.csv(result, file = geocoded.csv) Now I got the data cleaned, gecoded and ready to plot it on the map. Another task \u0026hellip;another library. Here I have used the leaflet library to add the basemap,plot the points , add clusters and markers to it. All it took was a couple of lines in R !! As a continuation to this project.\nlibrary(leaflet) library(dplyr) lf \u0026lt;- read.csv(\u0026#34;beer_lat_long.csv\u0026#34;, stringsAsFactors = FALSE) # Brings in the file \u0026#39;ctlist.csv\u0026#39; map \u0026lt;- leaflet(lf) %\u0026gt;% addTiles(\u0026#39;http://{s}.basemaps.cartocdn.com/dark_all/{z}/{x}/{y}.png\u0026#39;, attribution = \u0026#39;Map tiles by \u0026lt;a href=\u0026#34;http://stamen.com\u0026#34;\u0026gt;Stamen Design\u0026lt;/a\u0026gt;, \u0026lt;a href=\u0026#34;http://creativecommons.org/licenses/by/3.0\u0026#34;\u0026gt;CC BY 3.0\u0026lt;/a\u0026gt; \u0026amp;mdash; Map data \u0026amp;copy; \u0026lt;a href=\u0026#34;http://www.openstreetmap.org/copyright\u0026#34;\u0026gt;OpenStreetMap\u0026lt;/a\u0026gt;\u0026#39;) map %\u0026gt;% setView(-95.712891, 37.09024, zoom = 5) #add cluster map %\u0026gt;% addMarkers( popup = paste( \u0026#34;Brewer:\u0026#34;, lf$company, \u0026#34;Beer Name:\u0026#34;, lf$name, \u0026#34;Rank:\u0026#34;, lf$rank ), clusterOptions = markerClusterOptions() ) \u0026lt;div class='myIframe' \u0026gt; \u0026lt;iframe width=\u0026quot;720\u0026quot; height=\u0026quot;600\u0026quot; src=\u0026quot;https://s3.amazonaws.com/www.stencilled.me/post/leaflethtml.html\u0026quot;\u0026gt;\u0026lt;/iframe\u0026gt; \u0026lt;/div\u0026gt; Sadly we dont see many breweries in the top list from Texas, but then that gives me more the reasons to travel and visit many breweries. As I was doing this project I did realize that when I want to show the leaflet map using R in an html page this converts all the data loaded and that makes the html file heavy to load. So going on next I would to try do some more advanced visualization and representation of the same data using leaflet library and html.\n","permalink":"http://www.stencilled.me/posts/2017-02-22-topbeers2016/","summary":"\u003cstyle\u003e\n\n.myIframe {\nposition: relative;\npadding-bottom: 65.25%;\npadding-top: 30px;\nheight: 0;\noverflow: auto;\n-webkit-overflow-scrolling:touch; //\u003c\u003c--- THIS IS THE KEY\nborder: solid black 1px;\n}\n.myIframe iframe {\nposition: absolute;\ntop: 0;\nleft: 0;\nwidth: 100%;\nheight: 100%;\n}\n\u003c/style\u003e\n\u003cp\u003eAfter completing my previous post on food I wanted to work on something which I have started to explore recently,craft beer.\nA friend of mine introduced me to a beer club membership prior to which I never knew anything beyond the Corona\u0026rsquo;s .\nThen began the collection and here it is, what  have so far.\u003c/p\u003e","title":"Top Beers in 2016."},{"content":"Recently I visited Austin and many of my friends had mentioned about the variety in food options here. So my wife and I decided to search for places to eat on the foursquare app. As a standard search filter with high rating we ended up at pretty good places and foursquare did alert us to checkins whenever we reached a place. Post the trip I wanted to see how many people do checkins using this app and how the checkins are correlated with the ratings.\nThe first step here is to get the data . So I started to play around with the foursquare API and started working around the URL on what category(food,places to see, etc) to get the data . The authentication process for the foursquare API was a bit tricky but with my google-fu (( and special mention to the GIS tribe ) I was able to get going. Below is how you would get the client id and client secret when you create a new app.\nThe idea was how to do it for many places across the country. So I decided to use R to scrap and clean the data. You can find the code here.\nlibrary(RJSONIO) library(RCurl) options(RCurlOptions = list(cainfo = system.file(\"CurlSSL\", \"cacert.pem\", package = \"RCurl\"))) # Obtained from http://notebook.gaslampmedia.com/download-zip-code-latitude-longitude-city-state-county-csv/ ll = read.csv('zip_codes_states.csv',sep=\",\",head=TRUE) clientid = \"ENTER YOUR CLIENT ID\" clientsecret = \"ENTER YOUR CLIENT SECRET\" venue_name = c() venue_lat = c() venue_long = c() venue_city = c() venue_state = c() venue_country = c() venue_checkins = c() venue_users = c() venue_hasMenu = c() venue_rating = c() venue_postalCode = c() venue_usersCount = c() venue_formattedAddress = c() # To go through the lat longs in the csv and get the data. for (i in 1:dim(ll)[1]) { lat = ll$latitude[i] long = ll$longitude[i] # Do query and parse results query = paste(\"https://api.foursquare.com/v2/venues/explore?client_id=\",clientid,\"\u0026client_secret=\",clientsecret,\"\u0026ll=\",lat,\",\",long,\"\u0026query=food\u0026v=20170131\",sep=\"\") result = getURL(query) data \u003c- fromJSON(result) # For each result, save a bunch of fields, you can tweak this to your liking if (length(data$response$groups[[1]]$items) \u003e 0) { for (r in 1:length(data$response$groups[[1]]$items)) { tmp = data$response$groups[[1]]$items[[r]]$venue venue_name = c(venue_name,tmp$name) venue_lat = c(venue_lat,tmp$location$lat) venue_long = c(venue_long,tmp$location$lng) venue_city = c(venue_city,tmp$location$city) venue_state = c(venue_state,tmp$location$state) venue_country = c(venue_country,tmp$location$country) venue_checkins = c(venue_checkins,tmp$stats[1]) venue_hasMenu = c(venue_hasMenu,tmp$hasMenu) venue_rating = c(venue_rating,tmp$rating) # venue_shortName = c(venue_shortName,tmp$shortName) } } } # To Save the raw output save(venue_name,venue_lat,venue_long,venue_city,venue_state,venue_country,venue_checkins,venue_hasMenu ,venue_rating ,file='venuesResult.RData') # put this into a dataframe data = as.data.frame(cbind(locationvar,venue_checkins,venue_name,venue_lat,venue_long,venue_checkins,venue_users)) # remove the duplicate results dsub = subset(data,!duplicated(data)) names(dsub) = c(\"latlong\",\"checkins\",\"name\",\"latitude\",\"longitude\") # Export to file to csv which can be used for the next step. write.csv(tabley,file = \"Austin_Foursquare.csv\") Once this was done the next part was to how do I visualize this data . Since I have been trying my hands on d3js I used the cleaned output from R in CSV format to display how checkins and ratings vary for these places using bubble chart.\nCheckins less than: Rating greater than: \u0026lt;script\u0026gt; src = \u0026quot;https://d3js.org/d3.v4.min.js\u0026quot; \u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- \u0026lt;link href=\u0026quot;//fonts.googleapis.com/css?family=Raleway:400,300,600\u0026quot; rel=\u0026quot;stylesheet\u0026quot; type=\u0026quot;text/css\u0026quot;\u0026gt; --\u0026gt; \u0026lt;script src=\u0026quot;https://cdnjs.cloudflare.com/ajax/libs/d3/4.4.4/d3.min.js\u0026quot; type=\u0026quot;text/JavaScript\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;https://cdnjs.cloudflare.com/ajax/libs/d3-queue/3.0.3/d3-queue.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;https://cdnjs.cloudflare.com/ajax/libs/topojson/2.2.0/topojson.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;!-- JS–––––––––––––––––––––––––––––––––––––––––––––––––– --\u0026gt; \u0026lt;script src=\u0026quot;https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; var svg = d3.select(\u0026quot;svg\u0026quot;), width = +svg.attr(\u0026quot;width\u0026quot;), height = +svg.attr(\u0026quot;height\u0026quot;); var format = d3.format(\u0026quot;,d\u0026quot;); var color = d3.scaleOrdinal(d3.schemeCategory10); var pack = d3.pack() .size([width, width]) .padding(1.5); var inputs = {}; d3.csv(\u0026quot;austin_fsq.csv\u0026quot;, function(d) { d.sno = +d.sno; return d; }, function(error, data) { if (error) throw error; d3.selectAll(\u0026quot;input\u0026quot;).on(\u0026quot;change\u0026quot;, function(){ inputs[this.id] = +this.value; console.log(inputs.myValue + \u0026quot;-\u0026quot; + inputs.myRating) if(inputs.myValue \u0026amp;\u0026amp; inputs.myRating){ var classes = data.filter(d =\u0026gt; d.value \u0026lt; inputs.myValue \u0026amp;\u0026amp; d.rating \u0026gt;= inputs.myRating); draw(classes); } }) function draw(classes) { d3.selectAll(\u0026quot;svg \u0026gt; *\u0026quot;).remove(); console.log(classes.length); var root = d3.hierarchy({ children: classes }) .sum(function(d) { return d.value; }) .each(function(d) { if (id = d.data.id) { var id, i = id.lastIndexOf(\u0026quot;.\u0026quot;); d.id = id; d.package = id.slice(0, i); d.class = id.slice(i + 1); } }); var node = svg.selectAll(\u0026quot;.node\u0026quot;) .data(pack(root).leaves()) .enter().append(\u0026quot;g\u0026quot;) .attr(\u0026quot;class\u0026quot;, \u0026quot;node\u0026quot;) .attr(\u0026quot;transform\u0026quot;, function(d) { return \u0026quot;translate(\u0026quot; + d.x + \u0026quot;,\u0026quot; + d.y + \u0026quot;)\u0026quot;; }); node.append(\u0026quot;circle\u0026quot;) .attr(\u0026quot;id\u0026quot;, function(d) { return d.id; }) .attr(\u0026quot;r\u0026quot;, function(d) { return d.r; }) .style(\u0026quot;fill\u0026quot;, function(d) { return color(d.package); }); node.append(\u0026quot;clipPath\u0026quot;) .attr(\u0026quot;id\u0026quot;, function(d) { return \u0026quot;clip-\u0026quot; + d.id; }) .append(\u0026quot;use\u0026quot;) .attr(\u0026quot;xlink:href\u0026quot;, function(d) { return \u0026quot;#\u0026quot; + d.id; }); node.append(\u0026quot;text\u0026quot;) .attr(\u0026quot;clip-path\u0026quot;, function(d) { return \u0026quot;url(#clip-\u0026quot; + d.id + \u0026quot;)\u0026quot;; }) .selectAll(\u0026quot;tspan\u0026quot;) .data(function(d) { return d.class.split(/(?=[A-Z][^A-Z])/g); }) .enter().append(\u0026quot;tspan\u0026quot;) .attr(\u0026quot;x\u0026quot;, 0) .attr(\u0026quot;y\u0026quot;, function(d, i, nodes) { return 13 + (i - nodes.length / 2 - 0.5) * 10; }) .text(function(d) { return d; }); node.append(\u0026quot;title\u0026quot;) .text(function(d) { return d.data.id + \u0026quot;\\n\u0026quot; + format(d.value); }); } }); \u0026lt;/script\u0026gt; ","permalink":"http://www.stencilled.me/posts/2017-02-15-foursquare/","summary":"\u003cp\u003eRecently I visited Austin and many of my friends had mentioned about the variety in food options here.\nSo my wife and I decided to search for places to eat on the foursquare app. As a standard search filter\nwith high rating we ended up at pretty good places and foursquare did alert us to checkins whenever we\nreached a place. Post the trip I wanted to see how many people do checkins using this app and how the checkins are\ncorrelated with the ratings.\u003c/p\u003e","title":"Where do people eat in Austin ??"},{"content":"Hello World !!! This is my first project using d3js. Being a GIS professional, visualization is always a part of job. I always wanted to learn different ways for visualizing data.Let it be a simple plot using R or a choropleth map using Arcmap .\nIn this project I am trying to display how the NFL season 2016-17 went about. Ofcourse was surprised to see the Cowboys doing so well. May be we got our own Tom Brady.I started working with the data grabbed from the NFL site for how the season went by each seasson. Below is the location for the each team.\nNow if you hover over each team you would see which all teams they played and how many wins they had as a label under the hovered team.\n","permalink":"http://www.stencilled.me/posts/2017-01-29-nfl/","summary":"\u003cp\u003eHello World !!! This is my first project using d3js. Being a GIS professional, visualization is always a part of job.\nI always wanted to learn different ways for visualizing data.Let it be a simple plot using R or a choropleth map using Arcmap .\u003c/p\u003e\n\u003cp\u003eIn this project I am trying to display how the NFL season 2016-17 went about. Ofcourse was surprised to see the Cowboys doing so well.\nMay be we got our own Tom Brady.I started working with the data grabbed from the NFL site for how the season went by each seasson.\nBelow is the location for the each team.\u003c/p\u003e","title":"NFL Season 2016-17"},{"content":"","permalink":"http://www.stencilled.me/firstpost/","summary":"","title":""},{"content":"Hello, I’m Senthil Thyagarajan. My journey began with a school stencil, sparking a passion for mapping that evolved into a career in data analytics. As the Analytics Director at Mekanism and the founder of Stencilled, I specialize in transforming complex data into clear, visual narratives that drive informed decision-making. I integrate the latest AI trends into real-time reporting and strategic insights, ensuring every project is both innovative and impactful.\nOutside of work, I’m a dedicated LEGO builder—currently immersed in Formula 1 sets—and an enthusiastic home cook who loves experimenting with new recipes. Based in New Jersey with my wife and two kids, I bring creativity, precision, and a touch of playfulness to everything I do.\nWelcome to my professional space, where data meets design and innovation shapes the future.\n","permalink":"http://www.stencilled.me/page/about/","summary":"Hello, I’m Senthil Thyagarajan. My journey began with a school stencil, sparking a passion for mapping that evolved into a career in data analytics. As the Analytics Director at Mekanism and the founder of Stencilled, I specialize in transforming complex data into clear, visual narratives that drive informed decision-making. I integrate the latest AI trends into real-time reporting and strategic insights, ensuring every project is both innovative and impactful.\nOutside of work, I’m a dedicated LEGO builder—currently immersed in Formula 1 sets—and an enthusiastic home cook who loves experimenting with new recipes.","title":""},{"content":"R-Bloggers\n","permalink":"http://www.stencilled.me/page/blogroll/","summary":"R-Bloggers","title":""},{"content":"2025 Content Calendar: Web Development in Ad Analytics Q1: Technical Foundations January: Ad Tech Infrastructure \u0026ldquo;Building a Custom Ad Analytics Dashboard: From Concept to Deployment\u0026rdquo;\nTechnical stack: React, D3.js, BigQuery Focus: Real-time ad performance visualization Target: Technical recruiters, engineering teams \u0026ldquo;Implementing Server-Side Tracking: A Developer\u0026rsquo;s Guide\u0026rdquo;\nTechnical details: Node.js, AWS Lambda, Google Analytics 4 Focus: Privacy-first tracking solutions Target: Privacy officers, technical leads \u0026ldquo;Creating an Ad Performance API: Best Practices\u0026rdquo;\nCode examples: Python, FastAPI, PostgreSQL Focus: Scalable data architecture Target: Backend developers, data engineers February: Data Visualization \u0026ldquo;Interactive Ad Performance Maps with D3.js\u0026rdquo;\nLive demo: Geographic campaign performance Focus: Visual storytelling with data Target: Creative directors, marketing teams \u0026ldquo;Building Real-Time Ad Performance Alerts\u0026rdquo;\nTechnical implementation: WebSocket, React, Python Focus: Automated monitoring systems Target: Operations teams, analysts \u0026ldquo;Custom GA4 Event Tracking Implementation\u0026rdquo;\nCode walkthrough: JavaScript, GTM Focus: Advanced tracking scenarios Target: Analytics teams, developers March: Integration \u0026amp; Automation \u0026ldquo;Automating Ad Performance Reports with Python\u0026rdquo;\nGitHub repo: Complete solution Focus: End-to-end automation Target: Data analysts, marketing teams \u0026ldquo;Building a Cross-Platform Ad Analytics Dashboard\u0026rdquo;\nTechnical architecture: Microservices, React Focus: Unified reporting solution Target: Product managers, technical leads \u0026ldquo;Implementing A/B Testing Infrastructure\u0026rdquo;\nCode examples: React, Redux, Python Focus: Technical implementation Target: Product teams, developers Q2: Advanced Applications April: Entertainment Industry Focus \u0026ldquo;Building a Streaming Content Performance Dashboard\u0026rdquo;\nTechnical stack: React, D3.js, AWS Focus: Content engagement metrics Target: Netflix, streaming platforms \u0026ldquo;Implementing Family-Friendly Ad Tracking\u0026rdquo;\nTechnical approach: Content classification, ML Focus: Brand safety automation Target: Lego, family brands \u0026ldquo;Creating Interactive Ad Performance Stories\u0026rdquo;\nImplementation: React, D3.js, animations Focus: Engaging data visualization Target: Creative teams, stakeholders May: Machine Learning Integration \u0026ldquo;Building an Ad Performance Prediction Model\u0026rdquo;\nTechnical implementation: Python, scikit-learn Focus: ML in ad analytics Target: Data science teams \u0026ldquo;Implementing Real-Time Ad Optimization\u0026rdquo;\nArchitecture: Python, AWS, React Focus: Automated optimization Target: Engineering teams \u0026ldquo;Creating an Ad Creative Analysis Tool\u0026rdquo;\nTechnical stack: Computer Vision, Python Focus: Creative performance analysis Target: Creative teams, analysts June: Scalability \u0026amp; Performance \u0026ldquo;Optimizing Ad Analytics Dashboards for Scale\u0026rdquo;\nTechnical details: React, Redux, caching Focus: Performance optimization Target: Engineering teams \u0026ldquo;Implementing Real-Time Ad Analytics at Scale\u0026rdquo;\nArchitecture: Kafka, Python, React Focus: High-volume data processing Target: Technical leads \u0026ldquo;Building a Global Ad Performance Dashboard\u0026rdquo;\nTechnical approach: CDN, React, Python Focus: International scalability Target: Global brands Q3: Industry-Specific Solutions July: Streaming Platform Analytics \u0026ldquo;Building a Content-Aware Ad Performance Dashboard\u0026rdquo;\nTechnical implementation: React, Python, ML Focus: Content context in ads Target: Streaming platforms \u0026ldquo;Implementing Cross-Platform Ad Tracking\u0026rdquo;\nTechnical stack: React Native, Python Focus: Unified tracking solution Target: Mobile teams \u0026ldquo;Creating an Ad Engagement Analysis Tool\u0026rdquo;\nImplementation: React, D3.js, Python Focus: User interaction analysis Target: UX teams August: Retail \u0026amp; E-commerce \u0026ldquo;Building a Product-Aware Ad Performance Dashboard\u0026rdquo;\nTechnical stack: React, Python, BigQuery Focus: Product performance tracking Target: E-commerce teams \u0026ldquo;Implementing Store Locator with Ad Performance\u0026rdquo;\nTechnical approach: React, Maps API Focus: Location-based analytics Target: Retail brands \u0026ldquo;Creating a Family Product Safety Monitor\u0026rdquo;\nImplementation: Python, ML, React Focus: Brand safety automation Target: Family brands September: Technical Leadership \u0026ldquo;Leading a Technical Ad Analytics Team\u0026rdquo;\nFocus: Team management, architecture Target: Engineering managers \u0026ldquo;Building vs. Buying Ad Analytics Solutions\u0026rdquo;\nTechnical comparison: Custom vs. vendor Focus: Decision framework Target: Technical leads \u0026ldquo;Creating a Technical Ad Analytics Roadmap\u0026rdquo;\nImplementation plan: Phased approach Focus: Strategic planning Target: Product managers Q4: Innovation \u0026amp; Future October: Emerging Technologies \u0026ldquo;Implementing AI-Powered Ad Creative Analysis\u0026rdquo;\nTechnical stack: Python, TensorFlow, React Focus: AI in ad analytics Target: Innovation teams \u0026ldquo;Building a Privacy-First Ad Analytics System\u0026rdquo;\nTechnical approach: Differential privacy Focus: Privacy compliance Target: Privacy teams \u0026ldquo;Creating an AR Ad Performance Dashboard\u0026rdquo;\nImplementation: React, Three.js Focus: AR advertising analytics Target: Innovation teams November: Industry Trends \u0026ldquo;The Future of Ad Analytics: Technical Implementation\u0026rdquo;\nTechnical predictions: Architecture Focus: Future trends Target: Technical leaders \u0026ldquo;Building for the Cookieless Future\u0026rdquo;\nTechnical approach: Server-side, ML Focus: Privacy changes Target: Engineering teams \u0026ldquo;Implementing Cross-Device Ad Tracking\u0026rdquo;\nTechnical stack: React, Python, ML Focus: Identity resolution Target: Technical teams December: Year in Review \u0026ldquo;Technical Ad Analytics: 2025 Review\u0026rdquo;\nFocus: Year\u0026rsquo;s technical developments Target: Industry professionals \u0026ldquo;Building the Future of Ad Analytics\u0026rdquo;\nTechnical roadmap: 2026 Focus: Future predictions Target: Technical leaders \u0026ldquo;The Full-Stack Ad Analytics Developer\u0026rdquo;\nCareer guide: Technical skills Focus: Career development Target: Developers, recruiters Content Types \u0026amp; Formats Technical Content Code repositories with documentation Interactive demos Technical architecture diagrams Video tutorials Live coding sessions Distribution Channels GitHub (code repositories) Technical blog posts LinkedIn articles YouTube tutorials Industry conferences Technical meetups Success Metrics Technical Impact GitHub stars/forks Code adoption Technical feedback Implementation examples Career Impact Technical recruiter engagement Industry speaking opportunities Technical consulting requests Job interview requests Content Performance Technical article engagement Code repository activity Video tutorial views Conference speaking invitations ","permalink":"http://www.stencilled.me/calendar/","summary":"2025 Content Calendar: Web Development in Ad Analytics Q1: Technical Foundations January: Ad Tech Infrastructure \u0026ldquo;Building a Custom Ad Analytics Dashboard: From Concept to Deployment\u0026rdquo;\nTechnical stack: React, D3.js, BigQuery Focus: Real-time ad performance visualization Target: Technical recruiters, engineering teams \u0026ldquo;Implementing Server-Side Tracking: A Developer\u0026rsquo;s Guide\u0026rdquo;\nTechnical details: Node.js, AWS Lambda, Google Analytics 4 Focus: Privacy-first tracking solutions Target: Privacy officers, technical leads \u0026ldquo;Creating an Ad Performance API: Best Practices\u0026rdquo;","title":"Content Calendar 2025: Web Development in Ad Analytics"}]